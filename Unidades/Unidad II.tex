\part{Unidad II}

\chapter{Gradiente de una función}

La gradiente es uno de los conceptos fundamentales del cálculo multivariable y del análisis numérico. Representa la dirección de mayor crecimiento de una función escalar y constituye una herramienta esencial en optimización, análisis de modelos matemáticos y métodos iterativos numéricos \parencite{anton2012, stewart2016}. Su aplicación se extiende a la física, ingeniería, economía, aprendizaje automático y solución de ecuaciones diferenciales \parencite{riley2006, burden2016}.

\section{Definición}

Sea una función escalar de varias variables
\[
f(x_1, x_2, \dots, x_n),
\]
que es diferenciable en un punto del dominio. La \textbf{gradiente} de $f$, denotada por $\nabla f$ o $\mathrm{grad}\, f$, se define como el vector cuyas componentes son las derivadas parciales primeras de $f$:

\[
\nabla f(x_1, \ldots, x_n)
=
\left(
\frac{\partial f}{\partial x_1},
\frac{\partial f}{\partial x_2},
\ldots,
\frac{\partial f}{\partial x_n}
\right).
\]

Este vector constituye una generalización natural del concepto de derivada en una dimensión, extendiéndolo a espacios de dimensión superior \parencite{anton2012, sullivan2004}.

\section{Interpretación geométrica}

El vector gradiente posee una interpretación geométrica fundamental:  
\[
\nabla f(\mathbf{x}) \; \text{apunta en la dirección de máximo crecimiento de } f.
\]

Además:

\begin{itemize}
	\item La magnitud $\|\nabla f\|$ indica la tasa máxima de cambio.
	\item Es perpendicular (normal) a las curvas (o superficies) de nivel de $f$, es decir,
	\[
	f(x_1, x_2, \dots, x_n) = c.
	\]
\end{itemize}

Esta propiedad es clave en optimización y geometría diferencial, donde el análisis de superficies y sus normales es crucial \parencite{riley2006, anton2012}.

\section{Derivada direccional}

La gradiente se relaciona directamente con la \textbf{derivada direccional}. Para un vector unitario $\mathbf{u}$, la derivada direccional de $f$ en la dirección de $\mathbf{u}$ se define como:

\[
D_{\mathbf{u}} f(\mathbf{x})
=
\nabla f(\mathbf{x}) \cdot \mathbf{u}.
\]

Esta expresión demuestra que la gradiente actúa como ``coeficiente de cambio'' de la función en cualquier dirección dada. De esta fórmula se deduce que $D_{\mathbf{u}} f$ es máximo cuando $\mathbf{u}$ es paralelo a $\nabla f$, lo que reafirma la interpretación geométrica \parencite{anton2012, riley2006}.

\section{Propiedades fundamentales}

\begin{itemize}
	\item \textbf{Linealidad:}
	\[
	\nabla (af + bg) = a \nabla f + b \nabla g.
	\]
	\item \textbf{Producto:}
	\[
	\nabla(fg) = f \nabla g + g \nabla f.
	\]
	\item \textbf{Cociente:}
	\[
	\nabla\left( \frac{f}{g} \right)
	=
	\frac{g \nabla f - f \nabla g}{g^2}.
	\]
	\item \textbf{Composición (regla de la cadena):}
	\[
	\nabla (f \circ \mathbf{g}) =
	J_{\mathbf{g}}^{T} \nabla f(\mathbf{g}(x)),
	\]
	donde $J_{\mathbf{g}}$ es la matriz Jacobiana.
\end{itemize}

Estas propiedades son esenciales en el diseño de algoritmos numéricos de optimización y métodos iterativos \parencite{burden2016, chapra2015}.

\section{Gradiente y optimización}

El gradiente es el pilar de numerosos métodos de optimización, tanto teóricos como computacionales:

\begin{itemize}
	\item \textbf{Condición de óptimo:}  
	Un punto crítico $\mathbf{x}^*$ cumple
	\[
	\nabla f(\mathbf{x}^*) = \mathbf{0}.
	\]
	\item \textbf{Método de descenso del gradiente:}  
	El movimiento en dirección opuesta al gradiente,
	\[
	\mathbf{x}_{k+1} = \mathbf{x}_k - \alpha_k \nabla f(\mathbf{x}_k),
	\]
	permite aproximar mínimos locales de manera iterativa.
	\item \textbf{Método de Newton multivariable:}  
	Utiliza gradiente y Hessiana:
	\[
	\mathbf{x}_{k+1} =
	\mathbf{x}_k
	- H_f^{-1}(\mathbf{x}_k)
	\nabla f(\mathbf{x}_k).
	\]
\end{itemize}

Estos métodos son fundamentales en ingeniería, estadística, ciencia de datos y aprendizaje automático \parencite{burden2016, chapra2015, riley2006}.

\section{Aplicaciones}

La gradiente aparece de forma natural en numerosas áreas:

\subsection{Física clásica y electromagnetismo.}
La fuerza conservativa se relaciona mediante:
\[
\mathbf{F} = - \nabla U,
\]
donde $U$ es la energía potencial \parencite{riley2006}.

\subsection{Ingeniería y modelado.}
Gradientes se utilizan en:
\begin{itemize}
	\item Modelos térmicos (flujo de calor).
	\item Dinámica de fluidos.
	\item Elasticidad de materiales.
	\item Procesos de control.
\end{itemize}

\subsection{Aprendizaje automático y regresión.}
Los algoritmos de entrenamiento, como regresión lineal, redes neuronales y SVM, utilizan métodos basados en gradiente para minimizar funciones de costo.

\subsection{Economía y análisis de funciones multivariables.}
El gradiente describe sensibilidades o tasas de variación respecto a variables económicas, como producción, precios o utilidades \parencite{stewart2016}.

\subsection{Métodos numéricos.}
En métodos iterativos para resolver ecuaciones no lineales y optimización se requiere calcular gradientes en cada iteración \parencite{burden2016, chapra2015}.

\section{Aplicación de la gradiente en R}

El siguiente código en R simula el proceso del gradiente descendente para la función \( f(x) = x^2 \), cuya derivada es \( f'(x) = 2x \).  
Se observa cómo los valores de \( x \) van disminuyendo gradualmente hasta aproximarse al punto mínimo en \( x = 0 \).

\begin{rcode}
# ------------------------------------------------------------
# Simulación y gráfico de f(x), f'(x) y gradiente en R
# grad guarda el "nuevo" valor de x: x(i) - n * f'(x(i))
# ------------------------------------------------------------

# Parámetro n
n <- 0.01

# Definimos la función f(x) = x^2 y su derivada f'(x) = 2x
f <- function(x) x^2
f_deriv <- function(x) 2 * x

# Valor inicial y número de iteraciones
x0 <- 3
iter <- 21

# Vectores vacíos
x <- numeric(iter)
fx <- numeric(iter)
fpx <- numeric(iter)
grad <- numeric(iter)

# Primer valor
x[1] <- x0

# Bucle de cálculo
for (i in 1:iter) {
	fx[i]  <- f(x[i])
	fpx[i] <- f_deriv(x[i])
	grad[i] <- x[i] - n * fpx[i]
	if (i < iter) {
		x[i + 1] <- grad[i]
	}
}

# Crear tabla
tabla <- data.frame(
xo  = x,
fx  = fx,
fpx = fpx,
grad = grad
)
print(tabla)

# Gráfico con ggplot2
if(!require(ggplot2)) install.packages("ggplot2", repos = "https://cloud.r-project.org")
if(!require(tidyr))  install.packages("tidyr",  repos = "https://cloud.r-project.org")

library(ggplot2)
library(tidyr)

datos_long <- tabla |>
pivot_longer(cols = c(fx, fpx, grad),
names_to = "variable",
values_to = "valor")

ggplot(datos_long, aes(x = xo, y = valor, color = variable)) +
geom_point(size = 2) +
geom_line(linewidth = 1) +
scale_color_manual(values = c("blue", "red", "green"),
labels = c("f(x)", "f'(x)", "gradiente (x actualizado)")) +
labs(title = "Comparación de f(x), f'(x) y gradiente (x actualizado)",
x = "x",
y = "Valor",
color = "Variable") +
theme_minimal(base_size = 13)
\end{rcode}

\textbf{Ejecución}

\begin{figure} [htbp]
	\centering
	\includegraphics[width=0.60\linewidth]{Figuras/Gradiente1.png}
	\includegraphics[width=0.60\linewidth]{Figuras/Gradiente2.png}
	\caption{Gradiente de una función en R}
	\label{Gradiente}
	
\end{figure}

\chapter{Diferenciación Numérica}

\section{Introducción}

La diferenciación numérica es una herramienta fundamental en el análisis numérico y en la computación científica. Su objetivo es aproximar derivadas de funciones cuando no es posible obtenerlas de manera analítica, o cuando la función se conoce únicamente a través de datos discretos \parencite{burden2016, chapra2015}. Este enfoque es ampliamente utilizado en ingeniería, física, análisis de datos, modelos matemáticos aplicados y simulaciones computacionales.

El problema central consiste en aproximar derivadas mediante expresiones que utilizan valores de la función evaluados en puntos cercanos. Dichas expresiones se obtienen mediante la expansión en series de Taylor y constituyen la base de los métodos de diferencias finitas \parencite{anton2012, riley2006}.

\section{Fundamento teórico}

\subsection{Series de Taylor}

Sea $f$ una función suficientemente diferenciable en un intervalo que contiene el punto $x$. La expansión de Taylor alrededor de $x$ permite escribir:

\[
f(x+h) = f(x) + h f'(x) + \frac{h^2}{2} f''(x)
+ \frac{h^3}{6} f^{(3)}(x) + \cdots,
\]

\[
f(x-h) = f(x) - h f'(x) + \frac{h^2}{2} f''(x)
- \frac{h^3}{6} f^{(3)}(x) + \cdots.
\]

Estas expresiones permiten deducir fórmulas para aproximar derivadas y analizar el error local de truncamiento asociado a cada aproximación \parencite{burden2016, atkinson2009}.

\section{Fórmulas de diferencias finitas}

Las fórmulas para aproximar derivadas se clasifican según la manera en que utilizan los valores de la función.

\subsection{Diferencia hacia adelante}

Usando la expansión de Taylor se obtiene la fórmula:

\[
f'(x) \approx \frac{f(x+h) - f(x)}{h},
\]

con un error dado por:

\[
E = O(h).
\]

Esta aproximación es de primer orden y suele utilizarse por su simplicidad computacional, aunque es menos precisa que otras alternativas \parencite{burden2016, chapra2015}.

\subsection{Diferencia hacia atrás}

De manera análoga, se obtiene:

\[
f'(x) \approx \frac{f(x) - f(x-h)}{h},
\]

con el mismo error $O(h)$. Esta fórmula se utiliza cuando la evaluación hacia adelante no está disponible, como en métodos que avanzan en el tiempo y solo tienen datos previos \parencite{chapra2015}.

\subsection{Diferencia centrada}

Restando las expansiones de $f(x+h)$ y $f(x-h)$ se obtiene:

\[
f'(x) \approx \frac{f(x+h) - f(x-h)}{2h},
\]

con un error:

\[
E = O(h^2).
\]

Esta fórmula es más precisa porque utiliza información simétrica alrededor de $x$. Es ampliamente preferida en aplicaciones científicas donde se busca alta precisión \parencite{anton2012, riley2006}.

\section{Aproximaciones para derivadas de orden superior}

\subsection{Segunda derivada}

Sumando las expansiones de Taylor se obtiene:

\[
f''(x) \approx \frac{f(x+h) - 2f(x) + f(x-h)}{h^2},
\]

con error:

\[
E = O(h^2).
\]

Esta expresión es fundamental en la solución numérica de ecuaciones diferenciales, problemas de vibraciones, transferencia de calor y modelos físicos discretizados \parencite{riley2006, chapra2015}.

\subsection{Derivadas superiores}

Usando combinaciones de series de Taylor o matrices de diferencias finitas, es posible obtener fórmulas generales para derivadas de orden superior. Estas aproximaciones suelen presentar errores más sensibles al tamaño de paso y requieren mayor precisión numérica \parencite{burden2016, atkinson2009}.

\section{Análisis del error}

\subsection{Error de truncamiento}

El error de truncamiento proviene de ignorar términos de orden superior en la serie de Taylor. Para una fórmula dada, el orden del método indica cómo disminuye el error cuando $h$ se hace más pequeño. Por ejemplo:

\begin{itemize}
	\item Diferencia hacia adelante: $O(h)$
	\item Diferencia centrada: $O(h^2)$
	\item Segunda derivada centrada: $O(h^2)$
\end{itemize}

El análisis de error es esencial para comprender la estabilidad y precisión de los métodos numéricos \parencite{atkinson2009, burden2016}.

\subsection{Error de redondeo}

Cuando $h$ es demasiado pequeño, el error por cancelación y redondeo puede aumentar significativamente debido a las limitaciones de la aritmética de punto flotante. Esto genera un equilibrio óptimo entre reducir $h$ y mantener un nivel adecuado de estabilidad numérica \parencite{press2007, chapra2015}.

\section{Aplicaciones de la diferenciación numérica}

\subsection{Solución de ecuaciones diferenciales}

Las discretizaciones basadas en derivadas numéricas se utilizan en:

\begin{itemize}
	\item Ecuaciones diferenciales ordinarias (EDO).
	\item Ecuaciones diferenciales parciales (EDP).
	\item Modelos de dinámica de fluidos, calor, ondas y elasticidad.
\end{itemize}

Estas técnicas permiten transformar problemas continuos en sistemas algebraicos manejables computacionalmente \parencite{riley2006, press2007}.

\subsection{Optimización}

Los métodos de optimización requieren derivadas para estimar direcciones de búsqueda, calcular gradientes y aproximar Hessianas. En muchos casos se emplean derivadas numéricas cuando la función no es diferenciable analíticamente \parencite{burden2016}.

\subsection{Procesamiento de datos y señales}

En análisis de datos discretos se utilizan derivadas numéricas para:

\begin{itemize}
	\item detectar cambios bruscos,
	\item hallar picos en señales,
	\item calcular tasas de crecimiento,
	\item aproximar tendencias locales.
\end{itemize}

Estas aplicaciones son comunes en ingeniería, finanzas y ciencia experimental.

\section{Conclusiones}

La diferenciación numérica constituye una herramienta indispensable en las matemáticas aplicadas y la ingeniería. Su fundamento en series de Taylor, su flexibilidad en el uso de datos discretos y su integración con métodos de simulación y optimización la convierten en un componente esencial de la computación científica moderna. Su correcta aplicación requiere comprender tanto el comportamiento del error como la estabilidad de las fórmulas de aproximación \parencite{burden2016, chapra2015, atkinson2009}.

\chapter{Interpolación}

\section{Introducción}

La interpolación es una técnica fundamental del análisis numérico cuyo objetivo es construir una función que pase exactamente por un conjunto de puntos conocidos. A partir de datos discretos, se busca obtener una función aproximante que permita estimar valores intermedios, suavizar información o servir como base para futuros cálculos numéricos \parencite{burden2016, chapra2015}.

La interpolación es ampliamente utilizada en ingeniería, computación científica, procesamiento de datos, ciencias naturales, economía y muchas otras áreas donde los datos experimentales o simulados son comunes. Su fundamento matemático se basa en la existencia de un polinomio único de grado menor o igual a $n$ que pasa por $n+1$ puntos distintos \parencite{anton2012}.

\section{Fundamentos teóricos}

\subsection{Definición del problema}

Dado un conjunto de datos:

\[
(x_0, y_0), \; (x_1, y_1), \; \dots, \; (x_n, y_n)
\]

donde todos los $x_i$ son distintos, se desea encontrar una función $P(x)$ tal que:

\[
P(x_i) = y_i, \qquad i = 0,1,\dots,n.
\]

El tipo de función más comúnmente utilizado es un **polinomio interpolante**, debido a sus propiedades analíticas y su simplicidad computacional \parencite{atkinson2009}.

\section{Interpolación polinómica}

\subsection{Polinomio interpolante}

Existe un único polinomio de grado a lo sumo $n$ que interpola los $n+1$ puntos dados. Este resultado se conoce como el *Teorema de Interpolación Polinómica* \parencite{burden2016}.

Existen varias formas de construir este polinomio, cada una con ventajas computacionales específicas.

\section{Método de interpolación de Lagrange}

\subsection{Polinomio de Lagrange}

El polinomio de Lagrange se construye como:

\[
P(x) = \sum_{i=0}^n y_i L_i(x),
\]

donde:

\[
L_i(x) = \prod_{\substack{j=0 \\ j\neq i}}^{n} \frac{x - x_j}{x_i - x_j}.
\]

Cada $L_i(x)$ es un polinomio base que vale 1 en $x=x_i$ y 0 en los demás nodos.

Este método es útil teóricamente y tiene gran valor conceptual, aunque computacionalmente no es el más eficiente cuando se añaden nuevos puntos \parencite{burden2016, riley2006}.

\subsection{Error en el polinomio de Lagrange}

El error de interpolación está dado por la expresión:

\[
f(x) - P(x) =
\frac{f^{(n+1)}(\xi)}{(n+1)!}
\prod_{i=0}^n (x - x_i),
\]

para algún $\xi$ en el intervalo que contiene los nodos.  
Este resultado permite analizar cómo la distancia entre nodos y el grado del polinomio afectan la precisión \parencite{burden2016}.

\section{Interpolación por diferencias divididas de Newton}

\subsection{Diferencias divididas}

Las diferencias divididas son definidas recursivamente como:

\[
f[x_i] = y_i,
\]

\[
f[x_i, x_{i+1}] = \frac{f[x_{i+1}] - f[x_i]}{x_{i+1} - x_i},
\]

\[
f[x_i, x_{i+1}, x_{i+2}] = 
\frac{f[x_{i+1}, x_{i+2}] - f[x_i, x_{i+1}]}{x_{i+2} - x_i},
\]

y así sucesivamente.

\subsection{Polinomio de Newton}

El polinomio interpolante toma la forma:

\[
P(x) = a_0 + a_1 (x-x_0) + a_2 (x-x_0)(x-x_1)
+ \cdots + a_n \prod_{i=0}^{n-1} (x-x_i),
\]

donde los coeficientes $a_k$ son diferencias divididas:

\[
a_k = f[x_0, x_1, \dots, x_k].
\]

Esta formulación permite actualizar el polinomio fácilmente cuando se agrega un nuevo punto, lo cual la vuelve más eficiente que Lagrange en aplicaciones prácticas \parencite{chapra2015, burden2016}.

\subsection{Error del polinomio de Newton}

El error del polinomio de Newton coincide con el de Lagrange, pues ambos representan el mismo polinomio de interpolación:

\[
f(x) - P(x) =
\frac{f^{(n+1)}(\xi)}{(n+1)!}
\prod_{i=0}^{n} (x - x_i).
\]

\section{Problemas del polinomio global}

\subsection{Fenómeno de Runge}

Cuando se utilizan muchos puntos igualmente espaciados, el polinomio interpolante puede presentar oscilaciones significativas en los extremos del intervalo. Este comportamiento se conoce como el *fenómeno de Runge* y constituye una limitación importante de la interpolación polinómica global \parencite{atkinson2009, cheney2009}.

\subsection{Crecimiento del error para grados altos}

El error tiende a aumentar rápidamente cuando se incrementa el grado del polinomio debido a:

\begin{itemize}
	\item oscilaciones del polinomio,
	\item mala condición numérica,
	\item aumento de la magnitud de los términos del error.
\end{itemize}

Por este motivo, en muchas aplicaciones se prefiere dividir el intervalo en partes más pequeñas \parencite{burden2016}.

\section{Interpolación por tramos: splines}

\subsection{Concepto de spline}

Un *spline* es una función definida por tramos, donde cada tramo es típicamente un polinomio de bajo grado (por lo general grado 3). Su objetivo es evitar las oscilaciones del polinomio global manteniendo a la vez alta suavidad y precisión \parencite{press2007}.

\subsection{Spline cúbico}

El spline cúbico satisface:

\[
S_i(x_i) = y_i, \qquad S_i(x_{i+1}) = y_{i+1},
\]

y se impone que la función sea:

\begin{itemize}
	\item continua,
	\item con derivada primera continua,
	\item con derivada segunda continua,
\end{itemize}

en cada nodo.

Los splines son ampliamente utilizados en gráficos por computadora, diseño asistido por computadora, procesamiento de señales y simulaciones científicas \parencite{press2007, chapra2015}.

\section{Aplicaciones de la interpolación}

La interpolación es esencial en numerosas áreas:

\begin{itemize}
	\item reconstrucción de funciones a partir de datos discretos,
	\item procesamiento de señales,
	\item modelado y simulación numérica,
	\item gráficos y animación por computadora,
	\item soluciones aproximadas de ecuaciones diferenciales,
	\item generación de funciones suaves para análisis y optimización.
\end{itemize}

En ingeniería, por ejemplo, se utiliza para calibrar instrumentos, construir curvas de rendimiento o estimar valores no medidos experimentalmente \parencite{chapra2015, burden2016}.

\section{Conclusiones}

La interpolación constituye una herramienta esencial para el análisis de datos y la aproximación de funciones. Los métodos clásicos como Lagrange y Newton proporcionan una base sólida, mientras que los splines ofrecen mayor estabilidad y suavidad. La elección del método depende del número de puntos, la distribución de los mismos y la precisión requerida \parencite{burden2016, atkinson2009}.

\chapter{Valores y Vectores Propios}



\section{ ¿Qué son y por qué importan?}

Cuando estudiamos álgebra lineal, aprendemos que una matriz cuadrada $\mathbf{A}$ actúa como una función que transforma vectores. Generalmente, cuando una matriz multiplica a un vector, el resultado es un nuevo vector que ha cambiado tanto de dirección como de longitud.

Sin embargo, para casi todas las transformaciones lineales, existen ciertos vectores especiales que poseen una propiedad extraordinaria: no cambian su dirección al ser transformados por la matriz.

Imaginemos, por ejemplo, la rotación de un globo terráqueo. Casi todos los puntos en la superficie del globo se mueven a una nueva posición cuando este gira. Sin embargo, los puntos que están sobre el eje de rotación (el Polo Norte y el Polo Sur) no se desplazan lateralmente; permanecen sobre la misma línea. En el lenguaje del álgebra lineal, el eje de rotación representa un vector propio de esa transformación.

El término eigen proviene del alemán y significa "propio", "característico" o "innato". Por ello, a menudo se les llama valores y vectores característicos. Su importancia radica en que nos revelan los "ejes principales" o la estructura interna oculta de la matriz, simplificando problemas complejos de dinámica, vibraciones y análisis de datos \parencite{strang2016}.

\section{La transformación fundamental}

Matemáticamente, esta relación especial se define mediante una de las ecuaciones más famosas del álgebra lineal.

Sea $\mathbf{A}$ una matriz cuadrada de tamaño $n \times n$. Decimos que un vector $\mathbf{v}$ (distinto de cero) es un \textbf{vector propio} de $\mathbf{A}$ si se cumple la siguiente igualdad:

\[
\mathbf{A} \mathbf{v} = \lambda \mathbf{v}
\]

Donde:
\begin{itemize}
	\item $\mathbf{A}$ es la \textbf{matriz de transformación}. Representa la operación que estamos aplicando (rotación, estiramiento, corte, etc.).
	\item $\mathbf{v}$ es el \textbf{vector propio} (o eigenvector). Es el vector que mantiene su dirección original. Es importante notar que $\mathbf{v}$ no puede ser el vector nulo ($\mathbf{0}$).
	\item $\lambda$ (lambda) es el \textbf{valor propio} (o eigenvalor). Es un escalar (un número real o complejo) que indica cuánto se "estira" o "encoge" el vector $\mathbf{v}$.
\end{itemize}

\subsubsection{Interpretación geométrica}

La ecuación $\mathbf{A} \mathbf{v} = \lambda \mathbf{v}$ nos dice que la acción de la matriz $\mathbf{A}$ sobre el vector $\mathbf{v}$ es equivalente a simplemente multiplicar el vector por el número $\lambda$.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\linewidth]{Figuras/eigenvectors.png}
	\caption{Comparación de transformaciones. A la izquierda, un vector común cambia de dirección. A la derecha, un vector propio mantiene su dirección, solo cambia su longitud.}
	\label{fig:eigen_concept}
\end{figure}

Dependiendo del valor de $\lambda$, el efecto sobre el vector propio $\mathbf{v}$ puede ser:
\begin{itemize}
	\item Si $\lambda > 1$: El vector se estira.
	\item Si $0 < \lambda < 1$: El vector se contrae.
	\item Si $\lambda < 0$: El vector invierte su sentido (apunta al lado opuesto), pero se mantiene sobre la misma línea de acción.
	\item Si $\lambda = 0$: El vector se colapsa al origen (el sistema pierde una dimensión en esa dirección).
\end{itemize}

En resumen, los vectores propios son las "líneas de fuerza" naturales de la matriz, y los valores propios nos dicen con qué intensidad actúa la matriz sobre esas líneas \parencite{anton2012, riley2006}.

\section{Proceso de cálculo}

Calcular valores y vectores propios puede parecer un procedimiento mecánico, pero cada paso tiene una justificación lógica basada en la invertibilidad de las matrices. A continuación, desglosamos el algoritmo general para una matriz cuadrada $\mathbf{A}$ de tamaño $n \times n$.

\subsection{Paso 1: La ecuación característica}

Partimos de la definición fundamental:

\[
\mathbf{A} \mathbf{v} = \lambda \mathbf{v}.
\]

Para resolver esta ecuación, necesitamos agrupar los términos que contienen a $\mathbf{v}$ en un solo lado de la igualdad. Sin embargo, no podemos restar simplemente un escalar $\lambda$ de una matriz $\mathbf{A}$. Para hacerlos compatibles, multiplicamos $\lambda$ por la matriz identidad $\mathbf{I}$:

\[
\mathbf{A} \mathbf{v} - \lambda \mathbf{I} \mathbf{v} = \mathbf{0},
\]
\[
(\mathbf{A} - \lambda \mathbf{I}) \mathbf{v} = \mathbf{0}.
\]

Esta última expresión es un sistema de ecuaciones lineales homogéneo. Aquí surge un punto crucial:

\begin{itemize}
	\item Si la matriz $(\mathbf{A} - \lambda \mathbf{I})$ tuviera inversa, la única solución posible sería $\mathbf{v} = \mathbf{0}$ (la solución trivial).
	\item Como buscamos vectores propios \textbf{no nulos} ($\mathbf{v} \neq \mathbf{0}$), la matriz $(\mathbf{A} - \lambda \mathbf{I})$ debe ser \textbf{singular} (no invertible).
\end{itemize}

En álgebra lineal, una matriz es singular si y solo si su determinante es cero. Esto nos lleva a la \textbf{ecuación característica}:

\[
\det(\mathbf{A} - \lambda \mathbf{I}) = 0.
\]

Resolver esta ecuación es la clave para encontrar los valores propios \parencite{anton2012}

\subsection{Paso 2: Cálculo de los valores propios ($\lambda$)}

Al calcular el determinante $\det(\mathbf{A} - \lambda \mathbf{I})$, obtendremos un polinomio en función de $\lambda$ de grado $n$, conocido como el \textbf{polinomio característico}:

\[
p(\lambda) = (-1)^n \lambda^n + c_{n-1}\lambda^{n-1} + \dots + c_0 = 0.
\]

\textbf{Procedimiento:}
\begin{enumerate}
	\item Calcular el determinante de la matriz $(\mathbf{A} - \lambda \mathbf{I})$.
	\item Igualar el resultado a cero.
	\item Encontrar las raíces del polinomio (resolver para $\lambda$).
\end{enumerate}

Estas raíces son los valores propios de la matriz. Según el Teorema Fundamental del Álgebra, una matriz de $n \times n$ tendrá exactamente $n$ valores propios (contando sus multiplicidades y posibles valores complejos) \parencite{burden2016}.

\subsection{Paso 3: Cálculo de los vectores propios ($\mathbf{v}$)}

Una vez conocidos los valores de $\lambda$, debemos encontrar los vectores asociados a cada uno. Para cada valor propio $\lambda_i$ encontrado:

\begin{enumerate}
	\item Sustituimos $\lambda_i$ en la matriz $(\mathbf{A} - \lambda_i \mathbf{I})$.
	\item Resolvemos el sistema homogéneo:
	\[
	(\mathbf{A} - \lambda_i \mathbf{I}) \mathbf{v} = \mathbf{0}.
	\]
	\item El sistema tendrá infinitas soluciones (debido a que el determinante es cero). Debemos expresar la solución general en términos de vectores base.
\end{enumerate}

El conjunto de todos los vectores que satisfacen esta ecuación (más el vector cero) forma el \textbf{espacio propio} (o eigen-espacio) asociado a $\lambda_i$ .

\section{Ejemplo guiado}

Apliquemos el proceso a una matriz $2 \times 2$ para ilustrar los pasos. Sea:

\[
\mathbf{A} = \begin{pmatrix} 3 & 2 \\ 1 & 4 \end{pmatrix}.
\]

\subsection*{1. Construcción de la matriz característica}
Restamos $\lambda$ de la diagonal principal:

\[
\mathbf{A} - \lambda \mathbf{I} = \begin{pmatrix} 3-\lambda & 2 \\ 1 & 4-\lambda \end{pmatrix}.
\]

\subsection{2. Polinomio característico}
Calculamos el determinante e igualamos a cero:

\[
(3-\lambda)(4-\lambda) - (2)(1) = 0,
\]
\[
12 - 3\lambda - 4\lambda + \lambda^2 - 2 = 0,
\]
\[
\lambda^2 - 7\lambda + 10 = 0.
\]

\subsection*{3. Hallar las raíces ($\lambda$)}
Factorizamos la ecuación cuadrática:

\[
(\lambda - 5)(\lambda - 2) = 0.
\]

Los valores propios son \textbf{$\lambda_1 = 5$} y \textbf{$\lambda_2 = 2$}.

\subsection*{4. Hallar los vectores propios ($\mathbf{v}$)}

Caso $\lambda_1 = 5$
Sustituimos en $(\mathbf{A} - 5\mathbf{I})\mathbf{v} = \mathbf{0}$:

\[
\begin{pmatrix} -2 & 2 \\ 1 & -1 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}.
\]

Esto nos da la ecuación $-2x + 2y = 0$, o simplificando, $x = y$. Si elegimos $y=1$, entonces $x=1$.
\[
\mathbf{v}_1 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}.
\]

Caso $\lambda_2 = 2$:
Sustituimos en $(\mathbf{A} - 2\mathbf{I})\mathbf{v} = \mathbf{0}$:

\[
\begin{pmatrix} 1 & 2 \\ 1 & 2 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}.
\]

Esto nos da $x + 2y = 0$, o $x = -2y$. Si elegimos $y=1$, entonces $x=-2$.
\[
\mathbf{v}_2 = \begin{pmatrix} -2 \\ 1 \end{pmatrix}.
\]

\section{Propiedades y diagonalización}

Una de las utilidades más potentes de los valores y vectores propios es la capacidad de simplificar operaciones matriciales complejas. Si una matriz cuadrada puede ser interpretada como una transformación que solo estira vectores en ciertas direcciones, entonces podemos cambiar nuestro sistema de referencia para trabajar únicamente con estos estiramientos simples.

\subsection{Independencia lineal}

Para que una matriz $\mathbf{A}$ de tamaño $n \times n$ pueda ser simplificada completamente, necesita tener un conjunto completo de $n$ vectores propios linealmente independientes. Esto está garantizado siempre que la matriz tenga $n$ valores propios distintos.

En el caso de que existan valores propios repetidos (es decir, una raíz múltiple en el polinomio característico), es posible que no existan suficientes vectores propios independientes. Si esto ocurre, se dice que la matriz es \textit{defectuosa} y no puede diagonalizarse completamente, aunque puede aproximarse mediante la forma canónica de Jordan \parencite{anton2012}.

\subsection{El teorema de diagonalización}

Si la matriz $\mathbf{A}$ tiene $n$ vectores propios linealmente independientes, entonces es \textbf{diagonalizable}. Esto significa que $\mathbf{A}$ puede descomponerse en el producto de tres matrices específicas:

\[
\mathbf{A} = \mathbf{P} \mathbf{D} \mathbf{P}^{-1}
\]

Los componentes de esta factorización son:

\begin{itemize}
	\item $\mathbf{P}$: La \textbf{matriz de vectores propios}. Se construye colocando los vectores propios $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n$ como columnas de la matriz.
	\item $\mathbf{D}$: La \textbf{matriz diagonal}. Es una matriz donde todos los elementos fuera de la diagonal principal son cero. Los elementos de la diagonal son los valores propios $\lambda_1, \lambda_2, \dots, \lambda_n$, colocados en el mismo orden que sus correspondientes vectores en $\mathbf{P}$.
\end{itemize}

Esta igualdad ($\mathbf{D} = \mathbf{P}^{-1} \mathbf{A} \mathbf{P}$) implica que la matriz $\mathbf{A}$ es semejante a la matriz diagonal $\mathbf{D}$.

\subsection{Aplicación: Potencia de matrices}

Una consecuencia práctica inmediata de la diagonalización es el cálculo eficiente de potencias de matrices. Calcular $\mathbf{A}^k$ multiplicando la matriz por sí misma repetidamente es computacionalmente costoso ($O(n^3)$ por multiplicación) y propenso a acumular errores de redondeo.

Utilizando la descomposición espectral, observamos que:

\[
\mathbf{A}^2 = (\mathbf{P} \mathbf{D} \mathbf{P}^{-1}) (\mathbf{P} \mathbf{D} \mathbf{P}^{-1}) = \mathbf{P} \mathbf{D} (\mathbf{P}^{-1} \mathbf{P}) \mathbf{D} \mathbf{P}^{-1} = \mathbf{P} \mathbf{D}^2 \mathbf{P}^{-1}.
\]

Generalizando para cualquier potencia $k$:

\[
\mathbf{A}^k = \mathbf{P} \mathbf{D}^k \mathbf{P}^{-1}.
\]

La ventaja radica en que elevar una matriz diagonal a una potencia es trivial: simplemente se eleva cada elemento de la diagonal a la potencia $k$.

\[
\mathbf{D}^k = \begin{pmatrix}
	\lambda_1^k & 0 & \cdots & 0 \\
	0 & \lambda_2^k & \cdots & 0 \\
	\vdots & \vdots & \ddots & \vdots \\
	0 & 0 & \cdots & \lambda_n^k
\end{pmatrix}.
\]

Este atajo reduce drásticamente el costo de cómputo en algoritmos que requieren iteraciones matriciales, como las cadenas de Markov o las predicciones en sistemas dinámicos discretos \parencite{strang2016}.

\section{Implementación en R}

A continuación, se presenta el script completo para realizar el análisis espectral de la matriz $\mathbf{A}$ y utilizar sus propiedades para calcular potencias grandes de la misma.

\begin{rcode}
	A <- matrix(c(3, 1, 2, 4), nrow = 2, byrow = TRUE)
	
	sistema <- eigen(A)
	lambdas <- sistema$values
	V <- sistema$vectors
	
	print("Valores Propios:")
	print(lambdas)
	print("Vectores Propios:")
	print(V)
	
	k <- 20
	D_k <- diag(lambdas^k)
	P_inv <- solve(V)
	
	A_final <- V 
	
	print("Resultado de A elevado a la 20:")
	print(A_final)
\end{rcode}

\subsection{Explicación del código}

El código anterior se divide en tres etapas lógicas:

\begin{enumerate}
	\item \textbf{Definición y cálculo:} Primero definimos la matriz $\mathbf{A}$. La función nativa \texttt{eigen()} realiza todo el trabajo pesado, devolviendo una lista que separamos en \texttt{lambdas} (valores propios) y \texttt{V} (matriz de vectores propios).
	
	\item \textbf{Preparación de la diagonalización:} Para calcular $\mathbf{A}^{20}$, no multiplicamos la matriz por sí misma 20 veces. En su lugar, aplicamos la propiedad $\mathbf{A}^k = \mathbf{V}\mathbf{D}^k\mathbf{V}^{-1}$.
	\begin{itemize}
		\item Elevamos el vector \texttt{lambdas} a la potencia $k=20$ y lo convertimos en una matriz diagonal (\texttt{D\_k}).
		\item Calculamos la inversa de la matriz de vectores propios (\texttt{solve(V)}).
	\end{itemize}
	
	\item \textbf{Reconstrucción:} Finalmente, multiplicamos las tres matrices componentes ($\mathbf{V} \times \mathbf{D}^k \times \mathbf{V}^{-1}$) para obtener el resultado final de forma eficiente.
\end{enumerate}

\subsection{Visualización de resultados}

Al ejecutar el código, obtenemos los valores propios $\lambda_1 = 5$ y $\lambda_2 = 2$. Como se observa en la Figura \ref{fig:resultados_eigen}, la matriz final contiene valores extremadamente grandes (del orden de $10^{13}$).

Esto ocurre porque el término $5^{20}$ domina completamente la ecuación. Geométricamente, esto significa que cualquier vector transformado repetidamente por $\mathbf{A}$ terminará alineándose con el primer vector propio.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.7\linewidth]{Figuras/ResultadosEigen.png}
	\caption{Salida de la consola en R mostrando los valores propios y la matriz resultante $\mathbf{A}^{20}$.}
	\label{fig:resultados_eigen}
\end{figure}