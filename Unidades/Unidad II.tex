\part{Unidad II}

\chapter{Gradiente de una función}

La gradiente es uno de los conceptos fundamentales del cálculo multivariable y del análisis numérico. Representa la dirección de mayor crecimiento de una función escalar y constituye una herramienta esencial en optimización, análisis de modelos matemáticos y métodos iterativos numéricos \parencite{anton2012, stewart2016}. Su aplicación se extiende a la física, ingeniería, economía, aprendizaje automático y solución de ecuaciones diferenciales \parencite{riley2006, burden2016}.

\section{Definición}

Sea una función escalar de varias variables
\[
f(x_1, x_2, \dots, x_n),
\]
que es diferenciable en un punto del dominio. La \textbf{gradiente} de $f$, denotada por $\nabla f$ o $\mathrm{grad}\, f$, se define como el vector cuyas componentes son las derivadas parciales primeras de $f$:

\[
\nabla f(x_1, \ldots, x_n)
=
\left(
\frac{\partial f}{\partial x_1},
\frac{\partial f}{\partial x_2},
\ldots,
\frac{\partial f}{\partial x_n}
\right).
\]

Este vector constituye una generalización natural del concepto de derivada en una dimensión, extendiéndolo a espacios de dimensión superior \parencite{anton2012, sullivan2004}.

\section{Interpretación geométrica}

El vector gradiente posee una interpretación geométrica fundamental:  
\[
\nabla f(\mathbf{x}) \; \text{apunta en la dirección de máximo crecimiento de } f.
\]

Además:

\begin{itemize}
	\item La magnitud $\|\nabla f\|$ indica la tasa máxima de cambio.
	\item Es perpendicular (normal) a las curvas (o superficies) de nivel de $f$, es decir,
	\[
	f(x_1, x_2, \dots, x_n) = c.
	\]
\end{itemize}

Esta propiedad es clave en optimización y geometría diferencial, donde el análisis de superficies y sus normales es crucial \parencite{riley2006, anton2012}.

\section{Derivada direccional}

La gradiente se relaciona directamente con la \textbf{derivada direccional}. Para un vector unitario $\mathbf{u}$, la derivada direccional de $f$ en la dirección de $\mathbf{u}$ se define como:

\[
D_{\mathbf{u}} f(\mathbf{x})
=
\nabla f(\mathbf{x}) \cdot \mathbf{u}.
\]

Esta expresión demuestra que la gradiente actúa como ``coeficiente de cambio'' de la función en cualquier dirección dada. De esta fórmula se deduce que $D_{\mathbf{u}} f$ es máximo cuando $\mathbf{u}$ es paralelo a $\nabla f$, lo que reafirma la interpretación geométrica \parencite{anton2012, riley2006}.

\section{Propiedades fundamentales}

\begin{itemize}
	\item \textbf{Linealidad:}
	\[
	\nabla (af + bg) = a \nabla f + b \nabla g.
	\]
	\item \textbf{Producto:}
	\[
	\nabla(fg) = f \nabla g + g \nabla f.
	\]
	\item \textbf{Cociente:}
	\[
	\nabla\left( \frac{f}{g} \right)
	=
	\frac{g \nabla f - f \nabla g}{g^2}.
	\]
	\item \textbf{Composición (regla de la cadena):}
	\[
	\nabla (f \circ \mathbf{g}) =
	J_{\mathbf{g}}^{T} \nabla f(\mathbf{g}(x)),
	\]
	donde $J_{\mathbf{g}}$ es la matriz Jacobiana.
\end{itemize}

Estas propiedades son esenciales en el diseño de algoritmos numéricos de optimización y métodos iterativos \parencite{burden2016, chapra2015}.

\section{Gradiente y optimización}

El gradiente es el pilar de numerosos métodos de optimización, tanto teóricos como computacionales:

\begin{itemize}
	\item \textbf{Condición de óptimo:}  
	Un punto crítico $\mathbf{x}^*$ cumple
	\[
	\nabla f(\mathbf{x}^*) = \mathbf{0}.
	\]
	\item \textbf{Método de descenso del gradiente:}  
	El movimiento en dirección opuesta al gradiente,
	\[
	\mathbf{x}_{k+1} = \mathbf{x}_k - \alpha_k \nabla f(\mathbf{x}_k),
	\]
	permite aproximar mínimos locales de manera iterativa.
	\item \textbf{Método de Newton multivariable:}  
	Utiliza gradiente y Hessiana:
	\[
	\mathbf{x}_{k+1} =
	\mathbf{x}_k
	- H_f^{-1}(\mathbf{x}_k)
	\nabla f(\mathbf{x}_k).
	\]
\end{itemize}

Estos métodos son fundamentales en ingeniería, estadística, ciencia de datos y aprendizaje automático \parencite{burden2016, chapra2015, riley2006}.

\section{Aplicaciones}

La gradiente aparece de forma natural en numerosas áreas:

\subsection{Física clásica y electromagnetismo.}
La fuerza conservativa se relaciona mediante:
\[
\mathbf{F} = - \nabla U,
\]
donde $U$ es la energía potencial \parencite{riley2006}.

\subsection{Ingeniería y modelado.}
Gradientes se utilizan en:
\begin{itemize}
	\item Modelos térmicos (flujo de calor).
	\item Dinámica de fluidos.
	\item Elasticidad de materiales.
	\item Procesos de control.
\end{itemize}

\subsection{Aprendizaje automático y regresión.}
Los algoritmos de entrenamiento, como regresión lineal, redes neuronales y SVM, utilizan métodos basados en gradiente para minimizar funciones de costo.

\subsection{Economía y análisis de funciones multivariables.}
El gradiente describe sensibilidades o tasas de variación respecto a variables económicas, como producción, precios o utilidades \parencite{stewart2016}.

\subsection{Métodos numéricos.}
En métodos iterativos para resolver ecuaciones no lineales y optimización se requiere calcular gradientes en cada iteración \parencite{burden2016, chapra2015}.

\section{Aplicación de la gradiente en R}

El siguiente código en R simula el proceso del gradiente descendente para la función \( f(x) = x^2 \), cuya derivada es \( f'(x) = 2x \).  
Se observa cómo los valores de \( x \) van disminuyendo gradualmente hasta aproximarse al punto mínimo en \( x = 0 \).

\begin{rcode}
# ------------------------------------------------------------
# Simulación y gráfico de f(x), f'(x) y gradiente en R
# grad guarda el "nuevo" valor de x: x(i) - n * f'(x(i))
# ------------------------------------------------------------

# Parámetro n
n <- 0.01

# Definimos la función f(x) = x^2 y su derivada f'(x) = 2x
f <- function(x) x^2
f_deriv <- function(x) 2 * x

# Valor inicial y número de iteraciones
x0 <- 3
iter <- 21

# Vectores vacíos
x <- numeric(iter)
fx <- numeric(iter)
fpx <- numeric(iter)
grad <- numeric(iter)

# Primer valor
x[1] <- x0

# Bucle de cálculo
for (i in 1:iter) {
	fx[i]  <- f(x[i])
	fpx[i] <- f_deriv(x[i])
	grad[i] <- x[i] - n * fpx[i]
	if (i < iter) {
		x[i + 1] <- grad[i]
	}
}

# Crear tabla
tabla <- data.frame(
xo  = x,
fx  = fx,
fpx = fpx,
grad = grad
)
print(tabla)

# Gráfico con ggplot2
if(!require(ggplot2)) install.packages("ggplot2", repos = "https://cloud.r-project.org")
if(!require(tidyr))  install.packages("tidyr",  repos = "https://cloud.r-project.org")

library(ggplot2)
library(tidyr)

datos_long <- tabla |>
pivot_longer(cols = c(fx, fpx, grad),
names_to = "variable",
values_to = "valor")

ggplot(datos_long, aes(x = xo, y = valor, color = variable)) +
geom_point(size = 2) +
geom_line(linewidth = 1) +
scale_color_manual(values = c("blue", "red", "green"),
labels = c("f(x)", "f'(x)", "gradiente (x actualizado)")) +
labs(title = "Comparación de f(x), f'(x) y gradiente (x actualizado)",
x = "x",
y = "Valor",
color = "Variable") +
theme_minimal(base_size = 13)
\end{rcode}

\textbf{Ejecución}

\begin{figure} [H]
	\centering
	\includegraphics[width=0.60\linewidth]{Figuras/Gradiente1.png}
	\includegraphics[width=0.60\linewidth]{Figuras/Gradiente2.png}
	\caption{Gradiente de una función en R}
	\label{Gradiente}
	
\end{figure}

\section{Aplicación de la gradiente en un artículo de investigación}

En el artículo \textit{Comparing the Moore-Penrose Pesudoinverse and Gradient Descent for Solving Linear Regression Problems: A Performance Analysis} \parencite{adams2025_pseudoinverse_gradient} se comparan dos métodos fundamentales para resolver problemas de regresión lineal: el \textbf{pseudoinverso de Moore–Penrose} y el \textbf{descenso de gradiente}. La regresión lineal busca encontrar los parámetros que mejor predicen una variable dependiente a partir de variables independientes, minimizando los errores cuadrados (OLS). 

El pseudoinverso de Moore–Penrose ofrece una \textbf{solución analítica exacta} mediante operaciones matriciales, proporcionando el mínimo error posible. Sin embargo, puede ser computacionalmente costoso e inestable cuando la matriz de características es muy grande o está mal condicionada. En cambio, el \textbf{descenso de gradiente} es un método \textbf{iterativo} que ajusta los parámetros gradualmente en función de la dirección del gradiente negativo del error, controlado por una tasa de aprendizaje. Aunque no garantiza una solución exacta, es escalable y adaptable a grandes volúmenes de datos.

En el \textbf{marco teórico}, se explica que la estabilidad numérica y la eficiencia de cada método dependen del tamaño del conjunto de datos (\(n\)), el número de variables (\(d\)) y la condición de la matriz \(X\). El pseudoinverso utiliza descomposición SVD, mientras que el descenso de gradiente requiere pasos controlados y puede verse afectado por la escala de las variables.

En la \textbf{metodología}, el autor realizó experimentos con datos sintéticos y reales. Los datos sintéticos permitieron manipular parámetros como el número de muestras, la dimensionalidad y el factor de condición. Para los datos reales se usaron los conjuntos \textit{California Housing} (20,640 muestras y 8 variables) y \textit{Diabetes} (442 muestras y 10 variables). En ambos casos se evaluaron tres métricas: \textbf{tiempo de ejecución}, \textbf{error cuadrático medio (MSE)} y, para el descenso de gradiente, el \textbf{número de iteraciones hasta la convergencia}. Los experimentos se implementaron en Python, usando la función \texttt{pinv()} para el pseudoinverso y un algoritmo de gradiente con tasa de aprendizaje \(\alpha = 0.01\) y tolerancia \(10^{-6}\).

Los \textbf{resultados} mostraron que el pseudoinverso fue más rápido y preciso en conjuntos pequeños o moderados, manteniendo una alta estabilidad numérica. En cambio, el descenso de gradiente necesitó más iteraciones y su rendimiento dependió fuertemente del escalado de las variables y del valor de la tasa de aprendizaje. Sin embargo, en escenarios de alta dimensionalidad o grandes volúmenes de datos, el descenso de gradiente (y sus variantes como el estocástico) resultó más escalable y eficiente.

En \textbf{conclusión}, el pseudoinverso de Moore–Penrose es ideal para conjuntos de datos bien condicionados y de tamaño moderado, ofreciendo soluciones exactas con bajo error. El descenso de gradiente es preferible para problemas de gran escala, aunque requiere ajuste de hiperparámetros y un buen preprocesamiento. Ambos métodos son esenciales en el análisis de regresión, y su elección depende del equilibrio entre exactitud, estabilidad y costo computacional.

\chapter{Diferenciación Numérica}

\section{Introducción}

La diferenciación numérica es una herramienta fundamental en el análisis numérico y en la computación científica. Su objetivo es aproximar derivadas de funciones cuando no es posible obtenerlas de manera analítica, o cuando la función se conoce únicamente a través de datos discretos \parencite{burden2016, chapra2015}. Este enfoque es ampliamente utilizado en ingeniería, física, análisis de datos, modelos matemáticos aplicados y simulaciones computacionales.

El problema central consiste en aproximar derivadas mediante expresiones que utilizan valores de la función evaluados en puntos cercanos. Dichas expresiones se obtienen mediante la expansión en series de Taylor y constituyen la base de los métodos de diferencias finitas \parencite{anton2012, riley2006}.

\section{Fundamento teórico}

\subsection{Series de Taylor}

Sea $f$ una función suficientemente diferenciable en un intervalo que contiene el punto $x$. La expansión de Taylor alrededor de $x$ permite escribir:

\[
f(x+h) = f(x) + h f'(x) + \frac{h^2}{2} f''(x)
+ \frac{h^3}{6} f^{(3)}(x) + \cdots,
\]

\[
f(x-h) = f(x) - h f'(x) + \frac{h^2}{2} f''(x)
- \frac{h^3}{6} f^{(3)}(x) + \cdots.
\]

Estas expresiones permiten deducir fórmulas para aproximar derivadas y analizar el error local de truncamiento asociado a cada aproximación \parencite{burden2016, atkinson2009}.

\section{Fórmulas de diferencias finitas}

Las fórmulas para aproximar derivadas se clasifican según la manera en que utilizan los valores de la función.

\subsection{Diferencia hacia adelante}

Usando la expansión de Taylor se obtiene la fórmula:

\[
f'(x) \approx \frac{f(x+h) - f(x)}{h},
\]

con un error dado por:

\[
E = O(h).
\]

Esta aproximación es de primer orden y suele utilizarse por su simplicidad computacional, aunque es menos precisa que otras alternativas \parencite{burden2016, chapra2015}.

\subsection{Diferencia hacia atrás}

De manera análoga, se obtiene:

\[
f'(x) \approx \frac{f(x) - f(x-h)}{h},
\]

con el mismo error $O(h)$. Esta fórmula se utiliza cuando la evaluación hacia adelante no está disponible, como en métodos que avanzan en el tiempo y solo tienen datos previos \parencite{chapra2015}.

\subsection{Diferencia centrada}

Restando las expansiones de $f(x+h)$ y $f(x-h)$ se obtiene:

\[
f'(x) \approx \frac{f(x+h) - f(x-h)}{2h},
\]

con un error:

\[
E = O(h^2).
\]

Esta fórmula es más precisa porque utiliza información simétrica alrededor de $x$. Es ampliamente preferida en aplicaciones científicas donde se busca alta precisión \parencite{anton2012, riley2006}.

\section{Aproximaciones para derivadas de orden superior}

\subsection{Segunda derivada}

Sumando las expansiones de Taylor se obtiene:

\[
f''(x) \approx \frac{f(x+h) - 2f(x) + f(x-h)}{h^2},
\]

con error:

\[
E = O(h^2).
\]

Esta expresión es fundamental en la solución numérica de ecuaciones diferenciales, problemas de vibraciones, transferencia de calor y modelos físicos discretizados \parencite{riley2006, chapra2015}.

\subsection{Derivadas superiores}

Usando combinaciones de series de Taylor o matrices de diferencias finitas, es posible obtener fórmulas generales para derivadas de orden superior. Estas aproximaciones suelen presentar errores más sensibles al tamaño de paso y requieren mayor precisión numérica \parencite{burden2016, atkinson2009}.

\section{Análisis del error}

\subsection{Error de truncamiento}

El error de truncamiento proviene de ignorar términos de orden superior en la serie de Taylor. Para una fórmula dada, el orden del método indica cómo disminuye el error cuando $h$ se hace más pequeño. Por ejemplo:

\begin{itemize}
	\item Diferencia hacia adelante: $O(h)$
	\item Diferencia centrada: $O(h^2)$
	\item Segunda derivada centrada: $O(h^2)$
\end{itemize}

El análisis de error es esencial para comprender la estabilidad y precisión de los métodos numéricos \parencite{atkinson2009, burden2016}.

\subsection{Error de redondeo}

Cuando $h$ es demasiado pequeño, el error por cancelación y redondeo puede aumentar significativamente debido a las limitaciones de la aritmética de punto flotante. Esto genera un equilibrio óptimo entre reducir $h$ y mantener un nivel adecuado de estabilidad numérica \parencite{press2007, chapra2015}.

\section{Aplicaciones de la diferenciación numérica}

\subsection{Solución de ecuaciones diferenciales}

Las discretizaciones basadas en derivadas numéricas se utilizan en:

\begin{itemize}
	\item Ecuaciones diferenciales ordinarias (EDO).
	\item Ecuaciones diferenciales parciales (EDP).
	\item Modelos de dinámica de fluidos, calor, ondas y elasticidad.
\end{itemize}

Estas técnicas permiten transformar problemas continuos en sistemas algebraicos manejables computacionalmente \parencite{riley2006, press2007}.

\subsection{Optimización}

Los métodos de optimización requieren derivadas para estimar direcciones de búsqueda, calcular gradientes y aproximar Hessianas. En muchos casos se emplean derivadas numéricas cuando la función no es diferenciable analíticamente \parencite{burden2016}.

\subsection{Procesamiento de datos y señales}

En análisis de datos discretos se utilizan derivadas numéricas para:

\begin{itemize}
	\item detectar cambios bruscos,
	\item hallar picos en señales,
	\item calcular tasas de crecimiento,
	\item aproximar tendencias locales.
\end{itemize}

Estas aplicaciones son comunes en ingeniería, finanzas y ciencia experimental.

\section{Ejemplos}

\subsection{Ejemplo 1}

Un startup de tecnología registra el número acumulado de usuarios activos mensuales:

\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|c|}
		\hline
		Mes &  1 & 2 & 3 & 4 & 5 & 6 & 7 \\
		\hline
		Usuarios (miles) & 10 & 15 & 23 & 34 & 48 & 65& 85  \\
		\hline
	\end{tabular}
\end{table}

\textbf{Tareas}

\begin{enumerate}
	\item Calcula la tasa de crecimiento de usuarios (usuarios nuevos por mes) en el mes 4 usando diferencia centrada.
	\item Calcula la tasa de crecimiento en el mes 1 (usa diferencia adelante)
	\item Calcula la tasa de crecimiento en el mes 7 (usa diferencia atrás)
	\item ¿En qué mes fue mayor la aceleración del crecimiento? (calcula la sgeunda derivada en cada punto)
	\item Interpreta: ¿la startup está creciendo de forma acelerada o desacelerada?
\end{enumerate}

\textbf{Datos:}

\[
\begin{array}{c|ccccccc}
	\text{Mes} & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\
	\hline
	\text{Usuarios (miles)} & 10 & 15 & 23 & 34 & 48 & 65 & 85
\end{array}
\]

Sea $h=1$ mes (intervalo entre observaciones).

---

\textbf{1) Tasa de crecimiento en el mes 4 (diferencia centrada)}

\[
f'(x) \approx \frac{f(x+h) - f(x-h)}{2h}
\]

Para $x=4$:

\[
f'(4) = \frac{f(5) - f(3)}{2} = \frac{48 - 23}{2} = 12.5
\]

\[
\boxed{f'(4) = 12.5 \text{ miles de usuarios/mes}}
\]

---

\textbf{2) Tasa de crecimiento en el mes 1 (diferencia hacia adelante)}

\[
f'(x) \approx \frac{f(x+h) - f(x)}{h}
\]

Para $x=1$:

\[
f'(1) = f(2) - f(1) = 15 - 10 = 5
\]

\[
\boxed{f'(1) = 5 \text{ miles de usuarios/mes}}
\]

---

\textbf{3) Tasa de crecimiento en el mes 7 (diferencia hacia atrás)}

\[
f'(x) \approx \frac{f(x) - f(x-h)}{h}
\]

Para $x=7$:

\[
f'(7) = f(7) - f(6) = 85 - 65 = 20
\]

\[
\boxed{f'(7) = 20 \text{ miles de usuarios/mes}}
\]

---

\textbf{4) Aceleración del crecimiento (segunda derivada)}

\[
f''(x) \approx \frac{f(x+h) - 2f(x) + f(x-h)}{h^2}
\]

Para los puntos interiores ($x = 2,3,4,5,6$):

\[
\begin{aligned}
	f''(2) &= 23 - 2(15) + 10 = 3 \\
	f''(3) &= 34 - 2(23) + 15 = 3 \\
	f''(4) &= 48 - 2(34) + 23 = 3 \\
	f''(5) &= 65 - 2(48) + 34 = 3 \\
	f''(6) &= 85 - 2(65) + 48 = 3
\end{aligned}
\]

\[
\boxed{f''(x) = 3 \text{ miles de usuarios/mes}^2 \text{ (constante en los meses 2 a 6)}}
\]

---

\textbf{5) Interpretación}

Como la segunda derivada es positiva y constante, la tasa de crecimiento aumenta de forma continua.

\[
\boxed{\text{La startup crece de forma acelerada.}}
\]

En términos prácticos, cada mes la tasa de nuevos usuarios aumenta en aproximadamente $3$ mil usuarios/mes², lo que refleja un crecimiento sostenido y cada vez más rápido.

\textbf{Código en R}

\begin{rcode}
# Datos
mes <- 1:7
usuarios <- c(10, 15, 23, 34, 48, 65, 85)
h <- 1

# 1. Derivadas (tasa de crecimiento)
# Diferencia adelante, atrás y centrada
dif_adelante <- (usuarios[2] - usuarios[1]) / h
dif_atras <- (usuarios[7] - usuarios[6]) / h
dif_centrada <- (usuarios[5] - usuarios[3]) / (2*h)

cat("Tasa mes 1 (adelante):", dif_adelante, "miles/mes\n")
cat("Tasa mes 4 (centrada):", dif_centrada, "miles/mes\n")
cat("Tasa mes 7 (atrás):", dif_atras, "miles/mes\n")

# 2. Segunda derivada (aceleración) en puntos interiores
f2 <- numeric(length(usuarios))
for (i in 2:(length(usuarios)-1)) {
	f2[i] <- (usuarios[i+1] - 2*usuarios[i] + usuarios[i-1]) / (h^2)
}
f2

cat("\nSegunda derivada (aceleración) por mes:\n")
print(data.frame(mes=mes, aceleracion=f2))

# 3. Interpretación
if (all(f2[2:6] > 0)) {
	cat("\nLa aceleración es positiva → crecimiento acelerado.\n")
} else if (all(f2[2:6] < 0)) {
	cat("\nLa aceleración es negativa → crecimiento desacelerado.\n")
} else {
	cat("\nLa aceleración varía → crecimiento irregular.\n")
}
\end{rcode}

\textbf{Ejecución}

\begin{rcode}
> # Datos
> mes <- 1:7
> usuarios <- c(10, 15, 23, 34, 48, 65, 85)
> h <- 1
> 
> # 1. Derivadas (tasa de crecimiento)
> # Diferencia adelante, atrás y centrada
> dif_adelante <- (usuarios[2] - usuarios[1]) / h
> dif_atras <- (usuarios[7] - usuarios[6]) / h
> dif_centrada <- (usuarios[5] - usuarios[3]) / (2*h)
> 
> cat("Tasa mes 1 (adelante):", dif_adelante, "miles/mes\n")
Tasa mes 1 (adelante): 5 miles/mes
> cat("Tasa mes 4 (centrada):", dif_centrada, "miles/mes\n")
Tasa mes 4 (centrada): 12.5 miles/mes
> cat("Tasa mes 7 (atrás):", dif_atras, "miles/mes\n")
Tasa mes 7 (atrás): 20 miles/mes
> 
> # 2. Segunda derivada (aceleración) en puntos interiores
> f2 <- numeric(length(usuarios))
> for (i in 2:(length(usuarios)-1)) {
	+   f2[i] <- (usuarios[i+1] - 2*usuarios[i] + usuarios[i-1]) / (h^2)
	+ }
> f2
[1] 0 3 3 3 3 3 0
> 
> cat("\nSegunda derivada (aceleración) por mes:\n")

Segunda derivada (aceleración) por mes:
> print(data.frame(mes=mes, aceleracion=f2))
mes aceleracion
1   1           0
2   2           3
3   3           3
4   4           3
5   5           3
6   6           3
7   7           0
> 
> # 3. Interpretación
> if (all(f2[2:6] > 0)) {
	+   cat("\nLa aceleración es positiva → crecimiento acelerado.\n")
	+ } else if (all(f2[2:6] < 0)) {
	+   cat("\nLa aceleración es negativa → crecimiento desacelerado.\n")
	+ } else {
	+   cat("\nLa aceleración varía → crecimiento irregular.\n")
	+ }

La aceleración es positiva → crecimiento acelerado.
\end{rcode}

\subsection{Ejemplo 2}

Durante el entrenamiento de un modelo de Machine Learning, se registra la función de pérdida (loss) en cada época:

\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		Época & 0 & 10 & 20 & 30 & 40 & 50  \\
		\hline
		Loss & 2.45 & 1.82 & 1.35 & 1.08 & 0.95 & 0.89 \\
		\hline
	\end{tabular}
\end{table}

\textbf{Tareas}

\begin{enumerate}
	\item Calcula la tasa de cambio del loss en la época 20 usando diferencia centrada con h = 10.
	\item Calcula la segunda derivada en la época 30. ¿Qué indica sobre la convergencia?
	\item Si la tasa de cambio del loss es menor que 0.01 por época, el entrenamiento puede detenerse. ¿En qué época se alcanza este criterio?
	\item Estima el loss en la época 25 usando interpolación lineal basada en las derivadas.
\end{enumerate}

\textbf{Datos:}

\[
\begin{array}{c|cccccc}
	\text{Época} & 0 & 10 & 20 & 30 & 40 & 50 \\
	\hline
	\text{Loss} & 2.45 & 1.82 & 1.35 & 1.08 & 0.95 & 0.89
\end{array}
\]

Sea $h=10$ (épocas entre mediciones).

\bigskip

\textbf{1) Tasa de cambio del loss en la época 20 (diferencia centrada, $h=10$)}

La diferencia centrada con paso $h$:
\[
f'(x)\approx \frac{f(x+h)-f(x-h)}{2h}.
\]

Para $x=20$:
\[
f'(20)\approx \frac{f(30)-f(10)}{2\cdot 10}=\frac{1.08-1.82}{20}=\frac{-0.74}{20}=-0.037.
\]

\[
\boxed{f'(20)\approx -0.037\ \text{(loss por época)}}
\]
(es decir, la loss decrece aproximadamente $0.037$ por época).

\bigskip

\textbf{2) Segunda derivada en la época 30 (centrada, $h=10$)}

Fórmula de segunda derivada centrada:
\[
f''(x)\approx \frac{f(x+h)-2f(x)+f(x-h)}{h^2}.
\]

Para $x=30$:
\[
f''(30)\approx \frac{f(40)-2f(30)+f(20)}{10^2}
=\frac{0.95 - 2(1.08) + 1.35}{100}
=\frac{0.14}{100}=0.0014.
\]

\[
\boxed{f''(30)\approx 0.0014\ \text{(loss por época}^2\text{)}}
\]

\emph{Interpretación:} la segunda derivada es \emph{positiva} y pequeña, lo que indica que la pendiente (tasa de decremento del loss) se está volviendo menos negativa: la disminución del loss se está frenando — es decir, el entrenamiento está tendiendo a \emph{converger} (la curva se aplana).

\bigskip

\textbf{3) ¿En qué época la tasa de cambio del loss es menor que 0.01 por época?}

Calculamos las derivadas aproximadas en cada época usando diferencias hacia adelante/centradas/atrás con $h=10$:

\[
\begin{aligned}
	f'(0) &\approx \frac{f(10)-f(0)}{10}=\frac{1.82-2.45}{10}=-0.063 \\
	f'(10) &\approx \frac{f(20)-f(0)}{20}=\frac{1.35-2.45}{20}=-0.055 \\
	f'(20) &\approx -0.037 \quad(\text{ya calculado})\\
	f'(30) &\approx \frac{f(40)-f(20)}{20}=\frac{0.95-1.35}{20}=-0.020 \\
	f'(40) &\approx \frac{f(50)-f(30)}{20}=\frac{0.89-1.08}{20}=-0.0095 \\
	f'(50) &\approx \frac{f(50)-f(40)}{10}=\frac{0.89-0.95}{10}=-0.006
\end{aligned}
\]

El criterio es \(|f'|<0.01\). El primer punto donde se cumple es la \(\boxed{\text{época }40}\) (a partir de la época 40 la magnitud de la tasa cae por debajo de 0.01).

\bigskip

\textbf{4) Estima el loss en la época 25 usando interpolación lineal basada en las derivadas}

Usamos una aproximación lineal (expansión de primer orden) tomando el punto conocido en época 20 y la derivada en 20:
\[
f(25)\approx f(20) + f'(20)\cdot(25-20).
\]
Sustituyendo:
\[
f(25)\approx 1.35 + (-0.037)\cdot 5 = 1.35 - 0.185 = 1.165.
\]

\[
\boxed{f(25)\approx 1.165}
\]

\emph{Nota:} la interpolación lineal clásica entre los puntos (20, 1.35) y (30, 1.08) daría el punto medio
\(\, (1.35+1.08)/2 = 1.215\). La estimación basada en la derivada (linealización en 20) da \(1.165\); ambas son aproximaciones distintas (la elección depende del método que prefieras).

\textbf{Código en R}

\begin{rcode}
# Datos
epoca <- c(0,10,20,30,40,50)
loss  <- c(2.45, 1.82, 1.35, 1.08, 0.95, 0.89)
h <- 10

# Funciones para derivadas (usando h fijo = 10)
# Diferencia hacia adelante (para primer punto)
d_forward <- function(i) {
	(loss[i+1] - loss[i]) / h
}
# Diferencia hacia atrás (para último punto)
d_backward <- function(i) {
	(loss[i] - loss[i-1]) / h
}
# Diferencia centrada (para puntos interiores)
d_center <- function(i) {
	(loss[i+1] - loss[i-1]) / (2*h)
}
# Segunda derivada centrada
d2_center <- function(i) {
	(loss[i+1] - 2*loss[i] + loss[i-1]) / (h^2)
}

# Calcular derivadas en cada época
deriv <- numeric(length(loss))
for (i in seq_along(loss)) {
	if (i == 1) deriv[i] <- d_forward(i)
	else if (i == length(loss)) deriv[i] <- d_backward(i)
	else deriv[i] <- d_center(i)
}

# Calcular segunda derivada en puntos interiores
d2 <- rep(NA, length(loss))
for (i in 2:(length(loss)-1)) {
	d2[i] <- d2_center(i)
}

# Resultados pedidos:
cat("Derivadas (loss por época):\n")
print(data.frame(epoca=epoca, loss=loss, derivada=round(deriv, 5)))

cat("\nSegunda derivada (solo puntos interiores):\n")
print(data.frame(epoca=epoca, segunda_derivada=round(d2, 6)))

# 1) Tasa en época 20 (índice de época 3)
idx20 <- which(epoca == 20)
cat("\n1) f'(20) (centrada, h=10):", round(deriv[idx20], 5), " (loss/época)\n")

# 2) Segunda derivada en época 30
idx30 <- which(epoca == 30)
cat("2) f''(30):", round(d2[idx30], 6), " (loss/época^2)\n")

# Interpretación (impresa)
if (!is.na(d2[idx30])) {
	if (d2[idx30] > 0) {
		cat("   Interpretación: f''(30) > 0 → la magnitud de la pendiente se está reduciendo; la disminución del loss se está frenando (convergencia en curso).\n")
	} else if (d2[idx30] < 0) {
		cat("   Interpretación: f''(30) < 0 → la disminución se está acelerando.\n")
	} else {
		cat("   Interpretación: f''(30) ≈ 0 → curva cercana a lineal localmente.\n")
	}
}

# 3) ¿En qué época |f'| < 0.01 ?
idx_crit <- which(abs(deriv) < 0.01)
if (length(idx_crit) > 0) {
	cat("\n3) Épocas donde |f'| < 0.01:", epoca[idx_crit], "\n")
	cat("   Primera época que cumple el criterio:", epoca[min(idx_crit)], "\n")
} else {
	cat("\n3) No se alcanza |f'| < 0.01 en las épocas dadas.\n")
}

# 4) Estimación de loss en época 25 usando linearización en época 20:
t_target <- 25
t0 <- 20
f_t0 <- loss[which(epoca == t0)]
fprime_t0 <- deriv[which(epoca == t0)]
f25_est <- f_t0 + fprime_t0 * (t_target - t0)
cat("\n4) Estimación lineal (usando derivada en 20): f(25) ≈", round(f25_est, 4), "\n")

# Para comparación: interpolación lineal directa entre 20 y 30
f20 <- f_t0
f30 <- loss[which(epoca == 30)]
f25_lin_between <- f20 + (f30 - f20) * ( (t_target - 20) / (30 - 20) )
cat("   Interpolación lineal entre 20 y 30: f(25) ≈", round(f25_lin_between,4), "\n")
\end{rcode}

\textbf{Ejecución}

\begin{rcode}
> # Datos
> epoca <- c(0,10,20,30,40,50)
> loss  <- c(2.45, 1.82, 1.35, 1.08, 0.95, 0.89)
> h <- 10
> 
> # Funciones para derivadas (usando h fijo = 10)
> # Diferencia hacia adelante (para primer punto)
> d_forward <- function(i) {
	+   (loss[i+1] - loss[i]) / h
	+ }
> # Diferencia hacia atrás (para último punto)
> d_backward <- function(i) {
	+   (loss[i] - loss[i-1]) / h
	+ }
> # Diferencia centrada (para puntos interiores)
> d_center <- function(i) {
	+   (loss[i+1] - loss[i-1]) / (2*h)
	+ }
> # Segunda derivada centrada
> d2_center <- function(i) {
	+   (loss[i+1] - 2*loss[i] + loss[i-1]) / (h^2)
	+ }
> 
> # Calcular derivadas en cada época
> deriv <- numeric(length(loss))
> for (i in seq_along(loss)) {
	+   if (i == 1) deriv[i] <- d_forward(i)
	+   else if (i == length(loss)) deriv[i] <- d_backward(i)
	+   else deriv[i] <- d_center(i)
	+ }
> 
> # Calcular segunda derivada en puntos interiores
> d2 <- rep(NA, length(loss))
> for (i in 2:(length(loss)-1)) {
	+   d2[i] <- d2_center(i)
	+ }
> 
> # Resultados pedidos:
> cat("Derivadas (loss por época):\n")
Derivadas (loss por época):
> print(data.frame(epoca=epoca, loss=loss, derivada=round(deriv, 5)))
epoca loss derivada
1     0 2.45  -0.0630
2    10 1.82  -0.0550
3    20 1.35  -0.0370
4    30 1.08  -0.0200
5    40 0.95  -0.0095
6    50 0.89  -0.0060
> 
> cat("\nSegunda derivada (solo puntos interiores):\n")

Segunda derivada (solo puntos interiores):
> print(data.frame(epoca=epoca, segunda_derivada=round(d2, 6)))
epoca segunda_derivada
1     0               NA
2    10           0.0016
3    20           0.0020
4    30           0.0014
5    40           0.0007
6    50               NA
> 
> # 1) Tasa en época 20 (índice de época 3)
> idx20 <- which(epoca == 20)
> cat("\n1) f'(20) (centrada, h=10):", round(deriv[idx20], 5), " (loss/época)\n")

1) f'(20) (centrada, h=10): -0.037  (loss/época)
> 
> # 2) Segunda derivada en época 30
> idx30 <- which(epoca == 30)
> cat("2) f''(30):", round(d2[idx30], 6), " (loss/época^2)\n")
2) f''(30): 0.0014  (loss/época^2)
> 
> # Interpretación (impresa)
> if (!is.na(d2[idx30])) {
	+   if (d2[idx30] > 0) {
		+     cat("   Interpretación: f''(30) > 0 → la magnitud de la pendiente se está reduciendo; la disminución del loss se está frenando (convergencia en curso).\n")
		+   } else if (d2[idx30] < 0) {
		+     cat("   Interpretación: f''(30) < 0 → la disminución se está acelerando.\n")
		+   } else {
		+     cat("   Interpretación: f''(30) ≈ 0 → curva cercana a lineal localmente.\n")
		+   }
	+ }
Interpretación: f''(30) > 0 → la magnitud de la pendiente se está reduciendo; la disminución del loss se está frenando (convergencia en curso).
> 
> # 3) ¿En qué época |f'| < 0.01 ?
> idx_crit <- which(abs(deriv) < 0.01)
> if (length(idx_crit) > 0) {
	+   cat("\n3) Épocas donde |f'| < 0.01:", epoca[idx_crit], "\n")
	+   cat("   Primera época que cumple el criterio:", epoca[min(idx_crit)], "\n")
	+ } else {
	+   cat("\n3) No se alcanza |f'| < 0.01 en las épocas dadas.\n")
	+ }

3) Épocas donde |f'| < 0.01: 40 50 
Primera época que cumple el criterio: 40 
> 
> # 4) Estimación de loss en época 25 usando linearización en época 20:
> t_target <- 25
> t0 <- 20
> f_t0 <- loss[which(epoca == t0)]
> fprime_t0 <- deriv[which(epoca == t0)]
> f25_est <- f_t0 + fprime_t0 * (t_target - t0)
> cat("\n4) Estimación lineal (usando derivada en 20): f(25) ≈", round(f25_est, 4), "\n")

4) Estimación lineal (usando derivada en 20): f(25) ≈ 1.165 
> 
> # Para comparación: interpolación lineal directa entre 20 y 30
> f20 <- f_t0
> f30 <- loss[which(epoca == 30)]
> f25_lin_between <- f20 + (f30 - f20) * ( (t_target - 20) / (30 - 20) )
> cat("   Interpolación lineal entre 20 y 30: f(25) ≈", round(f25_lin_between,4), "\n")
Interpolación lineal entre 20 y 30: f(25) ≈ 1.215
\end{rcode}

\subsection{Ejemplo 3}

Una empresa de e-commerce registra sus ventas diarias (en miles de dólares) durante una semana de campaña:

\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|c|}
		\hline
		Día & Lun & Mar & Mié & Jue & Vie & sáb & Dom  \\
		\hline
		Ventas (\$k) & 45 & 52 & 61 & 58 & 73 & 89 & 95 \\
		\hline
	\end{tabular}
\end{table}

\textbf{Tareas}

\begin{enumerate}
	\item calcula la velocidad de crecimiento de ventas (derivada) para cada día usando diferencias finitas apropiadas.
	\item Identifica el día con mayor aceleración de ventas (segunda derivada positiva máxima)
	\item El jueves hubo una caída en la tendencia. Calcula la magnitud de esta desaceleración.
	\item Si la tendencia del domingo se mantiene, ¿Cuántas ventas esperarías el lunes siguiente? (usa extrapolación lineal con la derivada)
\end{enumerate}

\textbf{Datos:}

\[
\begin{array}{c|ccccccc}
	\text{Día} & \text{Lun} & \text{Mar} & \text{Mié} & \text{Jue} & \text{Vie} & \text{Sáb} & \text{Dom} \\
	\hline
	\text{Ventas }(\$k) & 45 & 52 & 61 & 58 & 73 & 89 & 95
\end{array}
\]

Sea $h=1$ día.

---

\textbf{1) Velocidad de crecimiento (primera derivada) — diferencias finitas}

Usamos diferencia hacia adelante en el primer día, hacia atrás en el último, y centrada en los puntos interiores:

\[
\begin{aligned}
	f'(1)\ (\text{Lun, adelante}) &= \frac{f(2)-f(1)}{1}=52-45=7 \\
	f'(2)\ (\text{Mar, centrada}) &= \frac{f(3)-f(1)}{2}=\frac{61-45}{2}=8 \\
	f'(3)\ (\text{Mié, centrada}) &= \frac{f(4)-f(2)}{2}=\frac{58-52}{2}=3 \\
	f'(4)\ (\text{Jue, centrada}) &= \frac{f(5)-f(3)}{2}=\frac{73-61}{2}=6 \\
	f'(5)\ (\text{Vie, centrada}) &= \frac{f(6)-f(4)}{2}=\frac{89-58}{2}=15.5 \\
	f'(6)\ (\text{Sáb, centrada}) &= \frac{f(7)-f(5)}{2}=\frac{95-73}{2}=11 \\
	f'(7)\ (\text{Dom, atrás}) &= \frac{f(7)-f(6)}{1}=95-89=6
\end{aligned}
\]

\[
\boxed{f' = [7,\ 8,\ 3,\ 6,\ 15.5,\ 11,\ 6]\ \ (\$k\ \text{por día})}
\]

---

\textbf{2) Aceleración (segunda derivada) y día con mayor aceleración positiva}

Segunda derivada centrada para puntos interiores:
\[
f''(x)\approx f(x+1)-2f(x)+f(x-1)
\]

Cálculos:
\[
\begin{aligned}
	f''(\text{Mar}) &= 61 - 2(52) + 45 = 2 \\
	f''(\text{Mié}) &= 58 - 2(61) + 52 = -12 \\
	f''(\text{Jue}) &= 73 - 2(58) + 61 = 18 \\
	f''(\text{Vie}) &= 89 - 2(73) + 58 = 1 \\
	f''(\text{Sáb}) &= 95 - 2(89) + 73 = -10
\end{aligned}
\]

\[
\boxed{f'' = [\text{NA},\ 2,\ -12,\ 18,\ 1,\ -10,\ \text{NA}]\ \ (\$k/\text{día}^2)}
\]

La segunda derivada máxima positiva es \(f''(\text{Jue})=18\), por lo que \(\boxed{\text{Jueves}}\) es el día con mayor aceleración positiva de ventas (magnitud \(18\ \$k/\text{día}^2\)).

---

\textbf{3) Caída en la tendencia el jueves — magnitud de la desaceleración}

Hay dos formas de cuantificar lo pedido:

\begin{itemize}
	\item \emph{Caída directa en ventas (valor absoluto):} las ventas bajaron de miércoles a jueves: \(58 - 61 = -3\). Es decir, hubo una \textbf{caída de \$3k} en ventas ese día.
	\item \emph{Desaceleración medida por la segunda derivada:} el valor \(f''(\text{Mié}) = -12\) indica una fuerte \textbf{desaceleración} alrededor del miércoles (la pendiente se hace más negativa allí). En cambio \(f''(\text{Jue})=+18\) porque inmediatamente después (viernes) las ventas suben mucho (73), lo que provoca una aceleración positiva en el punto jueves cuando se evalúa con el vecino viernes. 
\end{itemize}

Conclusión: la \textbf{caída instantánea} de ventas el jueves fue \(\boxed{3\ \$k}\). La \textbf{desaceleración local} más fuerte corresponde a \(f''(\text{Mié})=-12\ (\$k/\text{día}^2)\), indicando que justo en torno al miércoles la tendencia se frenó fuertemente.

---

\textbf{4) Extrapolación lineal: ventas el lunes siguiente}

Usamos la derivada en el domingo (diferencia atrás) como tasa diaria y extrapolamos 1 día:

\[
f(\text{Lun siguiente}) \approx f(\text{Dom}) + f'(\text{Dom})\cdot 1 = 95 + 6 = 101
\]

\[
\boxed{\text{Ventas esperadas el lunes siguiente } \approx 101\ \$k}
\]

---

Resumen numérico rápido:

\[
\begin{aligned}
	f' &: [7,\ 8,\ 3,\ 6,\ 15.5,\ 11,\ 6]\ (\$k/\text{día})\\
	f'' &: [\text{NA},\ 2,\ -12,\ 18,\ 1,\ -10,\ \text{NA}]\ (\$k/\text{día}^2)\\
	\text{Mayor aceleración positiva} &: \text{Jue }(18\ \$k/\text{día}^2)\\
	\text{Caída en Jue (ventas)} &: -3\ \$k\\
	\text{Proyección Lun siguiente} &: 101\ \$k
\end{aligned}
\]

\textbf{Código en R}

\begin{rcode}
# Datos
dias <- c("Lun","Mar","Mié","Jue","Vie","Sáb","Dom")
ventas <- c(45, 52, 61, 58, 73, 89, 95)  # en $k
h <- 1

# 1) Primera derivada: adelante en primer punto, atrás en último, centrada en interiores
deriv <- numeric(length(ventas))
for (i in seq_along(ventas)) {
	if (i == 1) {
		deriv[i] <- (ventas[i+1] - ventas[i]) / h
	} else if (i == length(ventas)) {
		deriv[i] <- (ventas[i] - ventas[i-1]) / h
	} else {
		deriv[i] <- (ventas[i+1] - ventas[i-1]) / (2*h)
	}
}

# 2) Segunda derivada (centrada) en puntos interiores
d2 <- rep(NA, length(ventas))
for (i in 2:(length(ventas)-1)) {
	d2[i] <- (ventas[i+1] - 2*ventas[i] + ventas[i-1]) / (h^2)
}

# Identificar día con mayor aceleración positiva
idx_max_acc <- which.max(d2)  # returns first max (NA treated as -Inf)
# but ensure it's interior (non-NA)
if (is.na(d2[idx_max_acc]) || length(idx_max_acc) == 0) {
	dia_max_acc <- NA
	max_acc <- NA
} else {
	dia_max_acc <- dias[idx_max_acc]
	max_acc <- d2[idx_max_acc]
}

# 3) Magnitud de la caída el jueves (drop Mié -> Jue)
drop_jue <- ventas[4] - ventas[3]  # index 4 = Jue, 3 = Mié

# 4) Extrapolación lineal para Lunes siguiente usando derivada en Domingo
f_dom <- ventas[length(ventas)]
fprime_dom <- deriv[length(ventas)]
f_lun_sig <- f_dom + fprime_dom * 1

# Mostrar resultados
cat("Primera derivada (ventas $k/día) por día:\n")
print(data.frame(dia=dias, ventas=ventas, derivada=deriv))

cat("\nSegunda derivada (ventas $k/día^2) en puntos interiores:\n")
print(data.frame(dia=dias, segunda_derivada=d2))

cat("\nDía con mayor aceleración positiva:\n")
cat("  ", dia_max_acc, "con f'' =", max_acc, "\n")

cat("\nCaída en ventas Mié -> Jue: ", drop_jue, " $k (si es negativo, es caída)\n")

cat("\nProyección lunes siguiente (usando derivada en domingo):\n")
cat("  Ventas domingo =", f_dom, " $k\n")
cat("  Tasa domingo =", fprime_dom, " $k/día\n")
cat("  Estimación lunes siguiente =", f_lun_sig, " $k\n")
\end{rcode}

\textbf{Ejecución}

\begin{rcode}
> # Datos
> dias <- c("Lun","Mar","Mié","Jue","Vie","Sáb","Dom")
> ventas <- c(45, 52, 61, 58, 73, 89, 95)  # en $k
> h <- 1
> 
> # 1) Primera derivada: adelante en primer punto, atrás en último, centrada en interiores
> deriv <- numeric(length(ventas))
> for (i in seq_along(ventas)) {
	+   if (i == 1) {
		+     deriv[i] <- (ventas[i+1] - ventas[i]) / h
		+   } else if (i == length(ventas)) {
		+     deriv[i] <- (ventas[i] - ventas[i-1]) / h
		+   } else {
		+     deriv[i] <- (ventas[i+1] - ventas[i-1]) / (2*h)
		+   }
	+ }
> 
> # 2) Segunda derivada (centrada) en puntos interiores
> d2 <- rep(NA, length(ventas))
> for (i in 2:(length(ventas)-1)) {
	+   d2[i] <- (ventas[i+1] - 2*ventas[i] + ventas[i-1]) / (h^2)
	+ }
> 
> # Identificar día con mayor aceleración positiva
> idx_max_acc <- which.max(d2)  # returns first max (NA treated as -Inf)
> # but ensure it's interior (non-NA)
> if (is.na(d2[idx_max_acc]) || length(idx_max_acc) == 0) {
	+   dia_max_acc <- NA
	+   max_acc <- NA
	+ } else {
	+   dia_max_acc <- dias[idx_max_acc]
	+   max_acc <- d2[idx_max_acc]
	+ }
> 
> # 3) Magnitud de la caída el jueves (drop Mié -> Jue)
> drop_jue <- ventas[4] - ventas[3]  # index 4 = Jue, 3 = Mié
> 
> # 4) Extrapolación lineal para Lunes siguiente usando derivada en Domingo
> f_dom <- ventas[length(ventas)]
> fprime_dom <- deriv[length(ventas)]
> f_lun_sig <- f_dom + fprime_dom * 1
> 
> # Mostrar resultados
> cat("Primera derivada (ventas $k/día) por día:\n")
Primera derivada (ventas $k/día) por día:
> print(data.frame(dia=dias, ventas=ventas, derivada=deriv))
dia ventas derivada
1 Lun     45      7.0
2 Mar     52      8.0
3 Mié     61      3.0
4 Jue     58      6.0
5 Vie     73     15.5
6 Sáb     89     11.0
7 Dom     95      6.0
> 
> cat("\nSegunda derivada (ventas $k/día^2) en puntos interiores:\n")

Segunda derivada (ventas $k/día^2) en puntos interiores:
> print(data.frame(dia=dias, segunda_derivada=d2))
dia segunda_derivada
1 Lun               NA
2 Mar                2
3 Mié              -12
4 Jue               18
5 Vie                1
6 Sáb              -10
7 Dom               NA
> 
> cat("\nDía con mayor aceleración positiva:\n")

Día con mayor aceleración positiva:
> cat("  ", dia_max_acc, "con f'' =", max_acc, "\n")
Jue con f'' = 18 
> 
> cat("\nCaída en ventas Mié -> Jue: ", drop_jue, " $k (si es negativo, es caída)\n")

Caída en ventas Mié -> Jue:  -3  $k (si es negativo, es caída)
> 
> cat("\nProyección lunes siguiente (usando derivada en domingo):\n")

Proyección lunes siguiente (usando derivada en domingo):
> cat("  Ventas domingo =", f_dom, " $k\n")
Ventas domingo = 95  $k
> cat("  Tasa domingo =", fprime_dom, " $k/día\n")
Tasa domingo = 6  $k/día
> cat("  Estimación lunes siguiente =", f_lun_sig, " $k\n")
Estimación lunes siguiente = 101  $k
\end{rcode}

\subsection{Ejemplo 4}

En redes neuronales, necesitas calcular el gradiente de la función sigmoide $\sigma(x) = \dfrac{1}{1 + e^{-x}}$ en varios puntos para el backpropagation. No tienes la derivada analítica programada.

Datos evaluados de la función:

\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|c|}
		\hline
		x & -3.0 & -2.0 & -1.0 & 0.0 & 1.0 & 2.0 & 3.0 \\
		\hline
		$\sigma(x)$ & 0.0474 & 0.1192 & 0.2689 & 0.5000 & 0.7311 & 0.8808 & 0.9526 \\ 
		\hline
	\end{tabular}
\end{table}

\textbf{Tareas}

\begin{enumerate}
	\item Calcula $\sigma'(0)$ usando diferencia centrada con h = 1.
	\item Calcula $\sigma'(-2)$ y $\sigma'(2)$ usando diferencias centradas.
	\item Compara tus resultados numéricos con la derivada analítica: $\sigma'(x) = \sigma(x)(1 - \sigma(x))$
	\item ¿Qué tamaño de h recomendarías para este cálculo? Justifica.
	\item ¿Por qué la derivada es simétrica alrededor de x = 0?
\end{enumerate}

\textbf{Datos:}

\[
\begin{array}{c|ccccccc}
	x & -3.0 & -2.0 & -1.0 & 0.0 & 1.0 & 2.0 & 3.0 \\
	\hline
	\sigma(x) & 0.0474 & 0.1192 & 0.2689 & 0.5000 & 0.7311 & 0.8808 & 0.9526
\end{array}
\]

Sea \(h=1\).

---

\textbf{1) Cálculo numérico de \(\sigma'(0)\) (diferencia centrada, \(h=1\))}

\[
\sigma'(x)\approx \frac{\sigma(x+h)-\sigma(x-h)}{2h}.
\]

Para \(x=0\):

\[
\sigma'(0)\approx \frac{\sigma(1)-\sigma(-1)}{2}
= \frac{0.7311 - 0.2689}{2} = \frac{0.4622}{2} = 0.2311.
\]

\[
\boxed{\sigma'(0)_{\text{num}}\approx 0.2311}
\]

---

\textbf{2) Cálculo numérico de \(\sigma'(-2)\) y \(\sigma'(2)\) (centradas, \(h=1\))}

Para \(x=-2\):

\[
\sigma'(-2)\approx \frac{\sigma(-1)-\sigma(-3)}{2}
= \frac{0.2689 - 0.0474}{2} = \frac{0.2215}{2} = 0.11075.
\]

Para \(x=2\):

\[
\sigma'(2)\approx \frac{\sigma(3)-\sigma(1)}{2}
= \frac{0.9526 - 0.7311}{2} = \frac{0.2215}{2} = 0.11075.
\]

\[
\boxed{\sigma'(-2)_{\text{num}} = \sigma'(2)_{\text{num}} \approx 0.11075}
\]

---

\textbf{3) Comparación con la derivada analítica}

La derivada analítica es
\[
\sigma'(x) = \sigma(x)\bigl(1-\sigma(x)\bigr).
\]

Calculando:

\[
\begin{aligned}
	\sigma'(0)_{\text{ana}} &= 0.5(1-0.5) = 0.25,\\[4pt]
	\sigma'(-2)_{\text{ana}} &= 0.1192(1-0.1192)=0.1192\cdot 0.8808 \approx 0.10499136,\\[4pt]
	\sigma'(2)_{\text{ana}} &= 0.8808(1-0.8808)=0.8808\cdot 0.1192 \approx 0.10499136.
\end{aligned}
\]

\textbf{Errores absolutos y relativos:}
\[
\begin{aligned}
	\text{En }x=0:&\quad \text{num}=0.2311,\ \text{ana}=0.25,\ \Delta=-0.0189\ (\approx -7.56\%)\\
	\text{En }x=\pm 2:&\quad \text{num}=0.11075,\ \text{ana}\approx 0.10499136,\ \Delta\approx 0.00575864\ (\approx +5.48\%)
\end{aligned}
\]

\emph{Comentario:} las aproximaciones centradas con \(h=1\) están cerca pero muestran errores de varios por ciento; la diferencia es mayor en \(x=0\) porque la curvatura local y la escala hacen que un paso grande (1) afecte más la precisión.

---

\textbf{4) Recomendación sobre el tamaño de \(h\)}

\begin{itemize}
	\item Si \emph{puedes evaluar la función analíticamente} (como la sigmoide), usa un \(h\) muy pequeño para las diferencias finitas — por ejemplo \(h\approx 10^{-5}\) a \(10^{-6}\) suele ser razonable en doble precisión, o usar la fórmula analítica directamente (lo ideal).
	\item Si solo dispones de \emph{valores tabulados} con separación discreta (aquí los datos están a paso 1), entonces el único \(h\) disponible para diferencias finitas sin interpolar es \(h=1\). Para mejorar la precisión necesitarías interpolar o ajustar una función (p. ej. ajustar la sigmoide) y luego derivar esa función.
	\item Regla práctica (análisis numérico): el error truncamiento ~ \(O(h^2)\) para la diferencia centrada, pero el error de redondeo aumenta al tomar \(h\) muy pequeño. Si usas la función analítica, elegir \(h\) en torno a \(\sqrt{\varepsilon}\cdot \text{escala}\) (donde \(\varepsilon\) es la máquina, ≈2.2e-16) da un buen compromiso; en práctica \(h\approx 10^{-5}\) suele funcionar bien.
\end{itemize}

---

\textbf{5) ¿Por qué la derivada es simétrica alrededor de \(x=0\)?}

La sigmoide cumple la relación
\[
\sigma(-x)=1-\sigma(x).
\]
Entonces
\[
\sigma'(-x)=\sigma(-x)\bigl(1-\sigma(-x)\bigr)=(1-\sigma(x))\bigl(1-(1-\sigma(x))\bigr)=\sigma(x)(1-\sigma(x))=\sigma'(x).
\]
Por tanto \(\sigma'(x)\) es una función par (even): \(\sigma'(-x)=\sigma'(x)\). Esa es la razón de la simetría alrededor de \(x=0\).

\textbf{Código en R}

\begin{rcode}
# Datos
x <- c(-3, -2, -1, 0, 1, 2, 3)
sigma <- c(0.0474, 0.1192, 0.2689, 0.5000, 0.7311, 0.8808, 0.9526)
h <- 1

# Indices helper
idx <- function(val) which(x == val)

# 1) Derivada centrada para x = 0, -2, 2
deriv_centered <- function(xval) {
	i <- idx(xval)
	# asumiendo que xval tiene vecinos (x-h) y (x+h) disponibles
	(sigma[i + 1] - sigma[i - 1]) / (2 * h)
}

d0_num <- deriv_centered(0)
dminus2_num <- deriv_centered(-2)
d2_num <- deriv_centered(2)

# 2) Derivada analítica sigma*(1-sigma)
d_analytic <- sigma * (1 - sigma)
d0_ana <- d_analytic[idx(0)]
dminus2_ana <- d_analytic[idx(-2)]
d2_ana <- d_analytic[idx(2)]

# 3) Errores
abs_err_0 <- d0_num - d0_ana
rel_err_0 <- abs_err_0 / d0_ana * 100

abs_err_m2 <- dminus2_num - dminus2_ana
rel_err_m2 <- abs_err_m2 / dminus2_ana * 100

abs_err_2 <- d2_num - d2_ana
rel_err_2 <- abs_err_2 / d2_ana * 100

# Mostrar resultados
cat("Resultados (h = 1):\n\n")

cat("x = 0:\n")
cat("  derivada num (centrada) =", d0_num, "\n")
cat("  derivada analítica      =", d0_ana, "\n")
cat("  error absoluto =", abs_err_0, ", error relativo (%) =", rel_err_0, "\n\n")

cat("x = -2:\n")
cat("  derivada num (centrada) =", dminus2_num, "\n")
cat("  derivada analítica      =", dminus2_ana, "\n")
cat("  error absoluto =", abs_err_m2, ", error relativo (%) =", rel_err_m2, "\n\n")

cat("x = 2:\n")
cat("  derivada num (centrada) =", d2_num, "\n")
cat("  derivada analítica      =", d2_ana, "\n")
cat("  error absoluto =", abs_err_2, ", error relativo (%) =", rel_err_2, "\n\n")

# Tabla resumen
res <- data.frame(
x = x,
sigma = sigma,
deriv_analitica = round(d_analytic, 8),
deriv_centrada = c(NA, dminus2_num, NA, d0_num, NA, d2_num, NA)
)
cat("Tabla resumen:\n")
print(res)

# Recomendación sobre h (comentario)
cat("\nNota: si puedes evaluar sigma(x) analíticamente, usa h pequeño (p.ej. 1e-5) o mejor aún usa la derivada analítica.\n")
\end{rcode}

\textbf{Ejecución}

\begin{rcode}
> # Datos
> x <- c(-3, -2, -1, 0, 1, 2, 3)
> sigma <- c(0.0474, 0.1192, 0.2689, 0.5000, 0.7311, 0.8808, 0.9526)
> h <- 1
> 
> # Indices helper
> idx <- function(val) which(x == val)
> 
> # 1) Derivada centrada para x = 0, -2, 2
> deriv_centered <- function(xval) {
	+   i <- idx(xval)
	+   # asumiendo que xval tiene vecinos (x-h) y (x+h) disponibles
	+   (sigma[i + 1] - sigma[i - 1]) / (2 * h)
	+ }
> 
> d0_num <- deriv_centered(0)
> dminus2_num <- deriv_centered(-2)
> d2_num <- deriv_centered(2)
> 
> # 2) Derivada analítica sigma*(1-sigma)
> d_analytic <- sigma * (1 - sigma)
> d0_ana <- d_analytic[idx(0)]
> dminus2_ana <- d_analytic[idx(-2)]
> d2_ana <- d_analytic[idx(2)]
> 
> # 3) Errores
> abs_err_0 <- d0_num - d0_ana
> rel_err_0 <- abs_err_0 / d0_ana * 100
> 
> abs_err_m2 <- dminus2_num - dminus2_ana
> rel_err_m2 <- abs_err_m2 / dminus2_ana * 100
> 
> abs_err_2 <- d2_num - d2_ana
> rel_err_2 <- abs_err_2 / d2_ana * 100
> 
> # Mostrar resultados
> cat("Resultados (h = 1):\n\n")
Resultados (h = 1):

> 
> cat("x = 0:\n")
x = 0:
> cat("  derivada num (centrada) =", d0_num, "\n")
derivada num (centrada) = 0.2311 
> cat("  derivada analítica      =", d0_ana, "\n")
derivada analítica      = 0.25 
> cat("  error absoluto =", abs_err_0, ", error relativo (%) =", rel_err_0, "\n\n")
error absoluto = -0.0189 , error relativo (%) = -7.56 

> 
> cat("x = -2:\n")
x = -2:
> cat("  derivada num (centrada) =", dminus2_num, "\n")
derivada num (centrada) = 0.11075 
> cat("  derivada analítica      =", dminus2_ana, "\n")
derivada analítica      = 0.1049914 
> cat("  error absoluto =", abs_err_m2, ", error relativo (%) =", rel_err_m2, "\n\n")
error absoluto = 0.00575864 , error relativo (%) = 5.48487 

> 
> cat("x = 2:\n")
x = 2:
> cat("  derivada num (centrada) =", d2_num, "\n")
derivada num (centrada) = 0.11075 
> cat("  derivada analítica      =", d2_ana, "\n")
derivada analítica      = 0.1049914 
> cat("  error absoluto =", abs_err_2, ", error relativo (%) =", rel_err_2, "\n\n")
error absoluto = 0.00575864 , error relativo (%) = 5.48487 

> 
> # Tabla resumen
> res <- data.frame(
+   x = x,
+   sigma = sigma,
+   deriv_analitica = round(d_analytic, 8),
+   deriv_centrada = c(NA, dminus2_num, NA, d0_num, NA, d2_num, NA)
+ )
> cat("Tabla resumen:\n")
Tabla resumen:
> print(res)
x  sigma deriv_analitica deriv_centrada
1 -3 0.0474      0.04515324             NA
2 -2 0.1192      0.10499136        0.11075
3 -1 0.2689      0.19659279             NA
4  0 0.5000      0.25000000        0.23110
5  1 0.7311      0.19659279             NA
6  2 0.8808      0.10499136        0.11075
7  3 0.9526      0.04515324             NA
> 
> # Recomendación sobre h (comentario)
> cat("\nNota: si puedes evaluar sigma(x) analíticamente, usa h pequeño (p.ej. 1e-5) o mejor aún usa la derivada analítica.\n")

Nota: si puedes evaluar sigma(x) analíticamente, usa h pequeño (p.ej. 1e-5) o mejor aún usa la derivada analítica.
\end{rcode}

\subsection{Ejemplo 5}

Un sistema de monitoreo registra el tiempo de respuesta (en ms) de una API cada hora durante un incidente:

\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
		\hline
		Hora & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7  \\
		\hline
		Latencia (ms) & 120 & 125 & 128 & 135 & 280 & 290 & 275 & 155 \\ 
		\hline
	\end{tabular}
\end{table}

\textbf{Tareas}

\begin{enumerate}
	\item Calcula la primera derivada (tasa de cambio) para cada hora usando diferencias finitas apropiadas.
	\item Identifica el momento del "pico de anomalía calculando dónde la segunda derivada cambia de signo (de positiva a negativa)
	\item Entre las horas 3 y 4 hay un salto brusco. Calcula la magnitud de este cambio.
	\item A partir de la hora 6, el sistema comienza a recuperarse. Calcula la tasa de recuperación (derivada negativa)
	\item Si defines una anomalía como un cambio mayor a 50 ms/hora, ¿en qué momentos se detectarían anomalías?
\end{enumerate}

\textbf{Datos:}

\[
\begin{array}{c|cccccccc}
	\text{Hora} & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\
	\hline
	\text{Latencia (ms)} & 120 & 125 & 128 & 135 & 280 & 290 & 275 & 155
\end{array}
\]

Sea $h=1$ hora.

---

\textbf{1) Primera derivada (tasa de cambio) — diferencias finitas}

Usamos diferencia hacia adelante en la hora 0, hacia atrás en la hora 7 y centrada en los puntos interiores:

\[
\begin{aligned}
	f'(0) &= \frac{f(1)-f(0)}{1}=125-120=5 \\
	f'(1) &= \frac{f(2)-f(0)}{2}=\frac{128-120}{2}=4 \\
	f'(2) &= \frac{f(3)-f(1)}{2}=\frac{135-125}{2}=5 \\
	f'(3) &= \frac{f(4)-f(2)}{2}=\frac{280-128}{2}=76 \\
	f'(4) &= \frac{f(5)-f(3)}{2}=\frac{290-135}{2}=77.5 \\
	f'(5) &= \frac{f(6)-f(4)}{2}=\frac{275-280}{2}=-2.5 \\
	f'(6) &= \frac{f(7)-f(5)}{2}=\frac{155-290}{2}=-67.5 \\
	f'(7) &= \frac{f(7)-f(6)}{1}=155-275=-120
\end{aligned}
\]

\[
\boxed{f' = [5,\ 4,\ 5,\ 76,\ 77.5,\ -2.5,\ -67.5,\ -120]\ \ (\text{ms/h})}
\]

---

\textbf{2) Pico de anomalía (cambio de signo en la segunda derivada)}

Segunda derivada centrada (para puntos interiores):
\[
f''(x)\approx f(x+1)-2f(x)+f(x-1)
\]

Cálculos:
\[
\begin{aligned}
	f''(1) &= 128 - 2(125) + 120 = -2 \\
	f''(2) &= 135 - 2(128) + 125 = 4 \\
	f''(3) &= 280 - 2(135) + 128 = 138 \\
	f''(4) &= 290 - 2(280) + 135 = -135 \\
	f''(5) &= 275 - 2(290) + 280 = -25 \\
	f''(6) &= 155 - 2(275) + 290 = -105
\end{aligned}
\]

La segunda derivada pasa de positiva (en $x=3$, $f''(3)=138$) a negativa (en $x=4$, $f''(4)=-135$). Por tanto el \emph{pico de anomalía} ocurre entre la \(\boxed{\text{hora }3\ \text{y}\ \text{hora }4}\), interpretándose el momento máximo del choque en la \(\boxed{\text{hora }4}\) (cuando la latencia alcanza 280 ms).

---

\textbf{3) Magnitud del salto brusco entre hora 3 y 4}

\[
\Delta = f(4) - f(3) = 280 - 135 = 145\ \text{ms}.
\]

\[
\boxed{\text{Salto brusco} = 145\ \text{ms}}
\]

---

\textbf{4) Tasa de recuperación a partir de la hora 6}

Usando la derivada calculada:

- Derivada centrada en hora 6: \(f'(6) = -67.5\ \text{ms/h}\) — tasa de recuperación promedio alrededor de la hora 6.  
- Entre hora 6 y 7 (derivada hacia atrás en 7): \(f'(7) = -120\ \text{ms/h}\) — recuperación más rápida en ese intervalo.

\[
\boxed{\text{Tasa de recuperación (aprox.) } \approx -67.5\ \text{ms/h (cent.) ; entre 6→7: }-120\ \text{ms/h}}
\]

---

\textbf{5) Detección de anomalías (|cambio| > 50 ms/h)}

Tomando los valores absolutos de \(f'\):
\[
|f'| = [5,\ 4,\ 5,\ 76,\ 77.5,\ 2.5,\ 67.5,\ 120]
\]
(he puesto 2.5 positivo para la magnitud en la hora 5).

Los instantes donde $|f'|>50$ son las horas: \(\boxed{3,\ 4,\ 6,\ 7}\) — es decir, se detectarían anomalías a partir de la hora 3 (inicio del ascenso brusco), en la hora 4 (máximo del salto), y durante la recuperación rápida en las horas 6 y 7.

---

\textbf{Resumen:}
\[
\begin{aligned}
	&f'=[5,4,5,76,77.5,-2.5,-67.5,-120]\ \text{(ms/h)} \\
	&f''(\text{interiores})=[-2,4,138,-135,-25,-105]\ \text{(ms/h}^2\text{)}\\
	&\text{Pico de anomalía: entre h=3 y h=4 (latencia máxima en h=4)}\\
	&\text{Salto 3→4: }145\ \text{ms}\\
	&\text{Tasa recuperación (h≈6): }-67.5\ \text{ms/h (cent.) ; 6→7: }-120\ \text{ms/h}\\
	&\text{Anomalías (|f'|>50): horas }3,4,6,7
\end{aligned}
\]

\textbf{Código en R}

\begin{rcode}
# Código R para calcular automáticamente lo anterior

# Datos
hora <- 0:7
latencia <- c(120, 125, 128, 135, 280, 290, 275, 155)
h <- 1

# 1) Primera derivada: adelante, centrada, atrás
fprime <- numeric(length(latencia))
for (i in seq_along(latencia)) {
	if (i == 1) {
		fprime[i] <- (latencia[i+1] - latencia[i]) / h          # forward
	} else if (i == length(latencia)) {
		fprime[i] <- (latencia[i] - latencia[i-1]) / h          # backward
	} else {
		fprime[i] <- (latencia[i+1] - latencia[i-1]) / (2*h)    # centered
	}
}

# 2) Segunda derivada centrada (puntos interiores)
f2 <- rep(NA, length(latencia))
for (i in 2:(length(latencia)-1)) {
	f2[i] <- (latencia[i+1] - 2*latencia[i] + latencia[i-1]) / (h^2)
}

# 3) Magnitud salto 3->4
salto_3_4 <- latencia[5] - latencia[4]  # indices: hora 4 is index 5, hora 3 is index 4

# 4) Tasas de recuperación (a partir de la hora 6)
idx6 <- which(hora == 6)
tasa_recuperacion_cent <- fprime[idx6]
# derivada entre 6->7 (forward from 6 or backward at 7)
tasa_6_7 <- (latencia[8] - latencia[7]) / h  # index 8=hora7, 7=hora6

# 5) Detección de anomalías: |f'| > 50 ms/h
umbral <- 50
anomalias_idx <- which(abs(fprime) > umbral)
anomalias_horas <- hora[anomalias_idx]

# Imprimir resultados
cat("Primera derivada f' (ms/h) por hora:\n")
print(data.frame(hora=hora, latencia=latencia, fprime=round(fprime,4)))

cat("\nSegunda derivada f'' (ms/h^2) en puntos interiores:\n")
print(data.frame(hora=hora, f2=f2))

cat("\nSalto brusco 3->4 (ms):", salto_3_4, "\n")

cat("\nTasa de recuperación (hora 6, centrada):", tasa_recuperacion_cent, "ms/h\n")
cat("Tasa entre 6->7:", tasa_6_7, "ms/h\n")

if (length(anomalias_idx) > 0) {
	cat("\nAnomalías detectadas (|f'| >", umbral, "ms/h) en horas:", anomalias_horas, "\n")
} else {
	cat("\nNo se detectaron anomalías con el umbral dado.\n")
}
\end{rcode}

\textbf{Ejecución}

\begin{rcode}
> # Código R para calcular automáticamente lo anterior
> 
> # Datos
> hora <- 0:7
> latencia <- c(120, 125, 128, 135, 280, 290, 275, 155)
> h <- 1
> 
> # 1) Primera derivada: adelante, centrada, atrás
> fprime <- numeric(length(latencia))
> for (i in seq_along(latencia)) {
	+   if (i == 1) {
		+     fprime[i] <- (latencia[i+1] - latencia[i]) / h          # forward
		+   } else if (i == length(latencia)) {
		+     fprime[i] <- (latencia[i] - latencia[i-1]) / h          # backward
		+   } else {
		+     fprime[i] <- (latencia[i+1] - latencia[i-1]) / (2*h)    # centered
		+   }
	+ }
> 
> # 2) Segunda derivada centrada (puntos interiores)
> f2 <- rep(NA, length(latencia))
> for (i in 2:(length(latencia)-1)) {
	+   f2[i] <- (latencia[i+1] - 2*latencia[i] + latencia[i-1]) / (h^2)
	+ }
> 
> # 3) Magnitud salto 3->4
> salto_3_4 <- latencia[5] - latencia[4]  # indices: hora 4 is index 5, hora 3 is index 4
> 
> # 4) Tasas de recuperación (a partir de la hora 6)
> idx6 <- which(hora == 6)
> tasa_recuperacion_cent <- fprime[idx6]
> # derivada entre 6->7 (forward from 6 or backward at 7)
> tasa_6_7 <- (latencia[8] - latencia[7]) / h  # index 8=hora7, 7=hora6
> 
> # 5) Detección de anomalías: |f'| > 50 ms/h
> umbral <- 50
> anomalias_idx <- which(abs(fprime) > umbral)
> anomalias_horas <- hora[anomalias_idx]
> 
> # Imprimir resultados
> cat("Primera derivada f' (ms/h) por hora:\n")
Primera derivada f' (ms/h) por hora:
> print(data.frame(hora=hora, latencia=latencia, fprime=round(fprime,4)))
hora latencia fprime
1    0      120    5.0
2    1      125    4.0
3    2      128    5.0
4    3      135   76.0
5    4      280   77.5
6    5      290   -2.5
7    6      275  -67.5
8    7      155 -120.0
> 
> cat("\nSegunda derivada f'' (ms/h^2) en puntos interiores:\n")

Segunda derivada f'' (ms/h^2) en puntos interiores:
> print(data.frame(hora=hora, f2=f2))
hora   f2
1    0   NA
2    1   -2
3    2    4
4    3  138
5    4 -135
6    5  -25
7    6 -105
8    7   NA
> 
> cat("\nSalto brusco 3->4 (ms):", salto_3_4, "\n")

Salto brusco 3->4 (ms): 145 
> 
> cat("\nTasa de recuperación (hora 6, centrada):", tasa_recuperacion_cent, "ms/h\n")

Tasa de recuperación (hora 6, centrada): -67.5 ms/h
> cat("Tasa entre 6->7:", tasa_6_7, "ms/h\n")
Tasa entre 6->7: -120 ms/h
> 
> if (length(anomalias_idx) > 0) {
	+   cat("\nAnomalías detectadas (|f'| >", umbral, "ms/h) en horas:", anomalias_horas, "\n")
	+ } else {
	+   cat("\nNo se detectaron anomalías con el umbral dado.\n")
	+ }

Anomalías detectadas (|f'| > 50 ms/h) en horas: 3 4 6 7
\end{rcode}

\subsection{Ejemplo 6}

Una campaña de marketing muestra la siguiente tasa de conversión (porcentaje) en función del gasto en publicidad (en miles de dólares):

\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		Gasto (\$k) & 0 & 5 & 10 & 15 & 20 & 25 \\
		\hline
		Conversión (\%) & 2.1 & 3.8 & 5.2 & 6.1 & 6.7 & 7.0 \\
		\hline
	\end{tabular}
\end{table}

\textbf{Tareas}

\begin{enumerate}
	\item Calcula el ROI marginal (derivada de conversión respecto al gasto) en cada punto usando diferencias centradas.
	\item ¿En qué momento de gasto el ROI marginal es mayor que 0.2\% por cada \$1000 invertido?
	\item La segunda derivada indica rendimientos decrecientes. Calcula la segunda derivada en \$15k.
	\item con base en las derivadas ¿recomendarías aumentar el gasto más allá de \$25k? Justifique matemáticamente.
\end{enumerate}

\textbf{Datos:}

\[
\begin{array}{c|cccccc}
	\text{Gasto }(\$k) & 0 & 5 & 10 & 15 & 20 & 25 \\
	\hline
	\text{Conversión }(\%) & 2.1 & 3.8 & 5.2 & 6.1 & 6.7 & 7.0
\end{array}
\]

Sea \(h=5\) (miles de dólares entre puntos).

---

\textbf{1) ROI marginal (derivada de conversión respecto al gasto)}

Usamos diferencia centrada para los puntos interiores y diferencias adelante/atrás en los extremos (paso \(h=5\)):

\[
f'(x)\approx\frac{f(x+h)-f(x-h)}{2h}
\]

Cálculos (en \% por \$1k):

\[
\begin{aligned}
	f'(0)\ (\text{adelante}) &= \frac{3.8-2.1}{5}=0.34 \\
	f'(5)\ (\text{centrada}) &= \frac{5.2-2.1}{10}=0.31 \\
	f'(10)\ (\text{centrada}) &= \frac{6.1-3.8}{10}=0.23 \\
	f'(15)\ (\text{centrada}) &= \frac{6.7-5.2}{10}=0.15 \\
	f'(20)\ (\text{centrada}) &= \frac{7.0-6.1}{10}=0.09 \\
	f'(25)\ (\text{atrás}) &= \frac{7.0-6.7}{5}=0.06
\end{aligned}
\]

\[
\boxed{f' = [0.34,\ 0.31,\ 0.23,\ 0.15,\ 0.09,\ 0.06]\ \ (\%\ \text{por}\ \$1\text{k})}
\]

---

\textbf{2) ¿Cuándo el ROI marginal \(>0.2\%\) por \$1k?}

Comparando los valores anteriores, las posiciones donde \(f'(x)>0.2\) son:

\[
x = 0,\ 5,\ 10\ (\$0k,\ \$5k,\ \$10k).
\]

Es decir, hasta \$10k el ROI marginal supera 0.2\%/\$1k.  
Si se desea el punto exacto donde \(f'(x)=0.2\) entre 10 y 15k, interpolando linealmente entre \(f'(10)=0.23\) y \(f'(15)=0.15\) obtenemos:

\[
x^* \approx 10 + \frac{0.23-0.20}{0.23-0.15}\cdot 5 \approx 11.875\ (\$k).
\]

Por tanto, para gasto mayor a \(\approx\$11.88\)k el ROI marginal cae por debajo de \(0.2\%\) por \$1k.

---

\textbf{3) Segunda derivada en \$15k (indicador de rendimientos decrecientes)}

La fórmula centrada de segunda derivada:
\[
f''(x)\approx \frac{f(x+h)-2f(x)+f(x-h)}{h^2}.
\]

Para \(x=15\) (con \(h=5\)):
\[
f''(15)\approx \frac{6.7 - 2(6.1) + 5.2}{5^2}
= \frac{6.7 -12.2 +5.2}{25} = \frac{-0.3}{25} = -0.012.
\]

\[
\boxed{f''(15) \approx -0.012\ \left(\%\ /\ (\$1\text{k})^2\right)}
\]

El valor negativo confirma \emph{rendimientos decrecientes} (la ganancia marginal de conversión disminuye con gasto).

---

\textbf{4) Recomendación sobre aumentar gasto más allá de \$25k (interpretación matemática)}

- La primera derivada en \$25k es \(f'(25)=0.06\%\) por \$1k — positiva pero muy pequeña.  
- La segunda derivada en los puntos interiores es negativa (ej. \(f''(15)\approx -0.012\)), lo que indica que el ROI marginal está decreciendo al aumentar el gasto: \(\;f'(\cdot)\) es decreciente.  
- Además, el umbral práctico que propusimos (0.2\%/\$1k) ya se supera sólo hasta \(\approx\$11.88\)k; más allá de eso la ganancia marginal cae por debajo de un valor atractivo.

Por estas razones matemáticas (ROI marginal pequeño y decreciente), \(\boxed{\text{no es recomendable aumentar el gasto mucho más allá de \$25k si el objetivo es maximizar la conversión por dólar adicional.}}\)  
Sin embargo, la decisión final puede depender de razones externas (p. ej. límites de escala, objetivos de branding, o expectativa de nonlinealidades fuera del rango observado).

---

\textbf{Resumen numérico}

\[
\begin{aligned}
	f' &: [0.34,\ 0.31,\ 0.23,\ 0.15,\ 0.09,\ 0.06]\ \%/\$1\text{k}\\
	f''(15) &\approx -0.012\ \%/(\$1\text{k})^2\\
	\text{Umbral }0.2\%/\$1\text{k} &\text{ alcanzado hasta } \$10\text{k} \ (\text{corte } \approx \$11.875\text{k})\\
	\text{Recomendación} &: \text{No aumentar significativamente más allá de \$25k (rendimientos decrecientes).}
\end{aligned}
\]

\textbf{Código en R}

\begin{rcode}
# Código R: cálculo automático de derivadas y decisiones
gasto <- c(0, 5, 10, 15, 20, 25)   # en $k
conv  <- c(2.1, 3.8, 5.2, 6.1, 6.7, 7.0)  # en %
h <- 5  # en $k

# 1) Derivadas (ROI marginal) - forward/back/centered
deriv <- numeric(length(conv))
for (i in seq_along(conv)) {
	if (i == 1) {
		deriv[i] <- (conv[i+1] - conv[i]) / h    # forward
	} else if (i == length(conv)) {
		deriv[i] <- (conv[i] - conv[i-1]) / h    # backward
	} else {
		deriv[i] <- (conv[i+1] - conv[i-1]) / (2*h)  # centered
	}
}

# 2) Segunda derivada centrada (interiores)
d2 <- rep(NA, length(conv))
for (i in 2:(length(conv)-1)) {
	d2[i] <- (conv[i+1] - 2*conv[i] + conv[i-1]) / (h^2)
}

# 3) ¿Dónde f'(x) > 0.2 % por $1k?
idx_gt_02 <- which(deriv > 0.2)
gasto_gt_02 <- gasto[idx_gt_02]

# punto aproximado donde f'(x)=0.2 (lineal entre 10 y 15 si desea precisión)
d10 <- deriv[which(gasto==10)]
d15 <- deriv[which(gasto==15)]
if (d10 != d15) {
	frac <- (d10 - 0.2) / (d10 - d15)
	x_threshold <- 10 + frac * (15 - 10)  # en $k
} else {
	x_threshold <- NA
}

# Impresión de resultados
cat("Gasto ($k)  :", gasto, "\n")
cat("Conversion % :", conv, "\n\n")
cat("ROI marginal f' (% per $1k):\n")
print(round(deriv, 4))

cat("\nSegunda derivada (interiores) f'' (% per ($1k)^2):\n")
print(round(d2, 5))

cat("\nPuntos donde f' > 0.2 (% per $1k):", gasto_gt_02, "\n")
cat("Umbral f'(x)=0.2 ocurre aproximadamente en gasto = ", round(x_threshold, 3), " $k\n\n")

# Recomendación basada en derivadas
cat("Interpretación:\n")
cat("- f'(25) =", round(deriv[length(deriv)],4), "%/ $1k  (marginal positivo pero bajo)\n")
cat("- f'' en interiores (ej. f''(15) =", round(d2[which(gasto==15)],5), ") indica rendimientos decrecientes\n")
cat("--> Matemáticamente no conviene aumentar demasiado el gasto más allá de 25k si la meta es obtener conversion % por dolar adicional.\n")
\end{rcode}

\textbf{Ejecución}

\begin{rcode}
> # Código R: cálculo automático de derivadas y decisiones
> gasto <- c(0, 5, 10, 15, 20, 25)   # en $k
> conv  <- c(2.1, 3.8, 5.2, 6.1, 6.7, 7.0)  # en %
> h <- 5  # en $k
> 
> # 1) Derivadas (ROI marginal) - forward/back/centered
> deriv <- numeric(length(conv))
> for (i in seq_along(conv)) {
	+   if (i == 1) {
		+     deriv[i] <- (conv[i+1] - conv[i]) / h    # forward
		+   } else if (i == length(conv)) {
		+     deriv[i] <- (conv[i] - conv[i-1]) / h    # backward
		+   } else {
		+     deriv[i] <- (conv[i+1] - conv[i-1]) / (2*h)  # centered
		+   }
	+ }
> 
> # 2) Segunda derivada centrada (interiores)
> d2 <- rep(NA, length(conv))
> for (i in 2:(length(conv)-1)) {
	+   d2[i] <- (conv[i+1] - 2*conv[i] + conv[i-1]) / (h^2)
	+ }
> 
> # 3) ¿Dónde f'(x) > 0.2 % por $1k?
> idx_gt_02 <- which(deriv > 0.2)
> gasto_gt_02 <- gasto[idx_gt_02]
> 
> # punto aproximado donde f'(x)=0.2 (lineal entre 10 y 15 si desea precisión)
> d10 <- deriv[which(gasto==10)]
> d15 <- deriv[which(gasto==15)]
> if (d10 != d15) {
	+   frac <- (d10 - 0.2) / (d10 - d15)
	+   x_threshold <- 10 + frac * (15 - 10)  # en $k
	+ } else {
	+   x_threshold <- NA
	+ }
> 
> # Impresión de resultados
> cat("Gasto ($k)  :", gasto, "\n")
Gasto ($k)  : 0 5 10 15 20 25 
> cat("Conversion % :", conv, "\n\n")
Conversion % : 2.1 3.8 5.2 6.1 6.7 7 

> cat("ROI marginal f' (% per $1k):\n")
ROI marginal f' (% per $1k):
> print(round(deriv, 4))
[1] 0.34 0.31 0.23 0.15 0.09 0.06
> 
> cat("\nSegunda derivada (interiores) f'' (% per ($1k)^2):\n")

Segunda derivada (interiores) f'' (% per ($1k)^2):
> print(round(d2, 5))
[1]     NA -0.012 -0.020 -0.012 -0.012     NA
> 
> cat("\nPuntos donde f' > 0.2 (% per $1k):", gasto_gt_02, "\n")

Puntos donde f' > 0.2 (% per $1k): 0 5 10 
> cat("Umbral f'(x)=0.2 ocurre aproximadamente en gasto = ", round(x_threshold, 3), " $k\n\n")
Umbral f'(x)=0.2 ocurre aproximadamente en gasto =  11.875  $k

> 
> # Recomendación basada en derivadas
> cat("Interpretación:\n")
Interpretación:
> cat("- f'(25) =", round(deriv[length(deriv)],4), "%/ $1k  (marginal positivo pero bajo)\n")
- f'(25) = 0.06 %/ $1k  (marginal positivo pero bajo)
> cat("- f'' en interiores (ej. f''(15) =", round(d2[which(gasto==15)],5), ") indica rendimientos decrecientes\n")
- f'' en interiores (ej. f''(15) = -0.012 ) indica rendimientos decrecientes
> cat("--> Matemáticamente no conviene aumentar demasiado el gasto más allá de 25k si la meta es obtener conversion % por dolar adicional.\n")
--> Matemáticamente no conviene aumentar demasiado el gasto más allá de 25k si la meta es obtener conversion % por dolar adicional.
\end{rcode}

\subsection{Ejemplo 7}

Tienes un dataset con la señal de un sensor de temperatura en un proceso industrial medida cada segundo:

\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
		\hline
		Tiempo (s) & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\
		Temp (°C) & 20.1 & 20.3 & 20.8 & 21.5 & 22.6 & 24.2 & 26.1 & 28.5 \\
		\hline
	\end{tabular}
\end{table}

\textbf{Tareas}

\begin{enumerate}
	\item Crea una nueva feature: la velocidad de cambio de temperatura (primera derivada en cada punto. 
	\item Crea otra feature: la aceleración del cambio (segunda derivada)
	\item Un aumento de temperatura mayor a 0.8°C/s indica un problema. ¿En qué momentos se detectaría esta alerta?
	\item Normaliza las features derivadas usando min-max scaling.
	\item Explica por qué estas features derivadas pueden ser útiles para un modelo de clasificación que detecta anomalías.
\end{enumerate}

\textbf{Datos:}

\[
\begin{array}{c|cccccccc}
	\text{Tiempo (s)} & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7\\
	\hline
	T(°C) & 20.1 & 20.3 & 20.8 & 21.5 & 22.6 & 24.2 & 26.1 & 28.5
\end{array}
\]

Sea \(h = 1\,\text{s}\).

---

\textbf{1) Velocidad de cambio de temperatura (primera derivada)}

\[
T'(t) \approx \frac{T(t+h)-T(t-h)}{2h}
\]

(con diferencias centradas; adelante y atrás en los extremos)

\[
\begin{aligned}
	T'(0)&=\frac{20.3-20.1}{1}=0.2\\
	T'(1)&=\frac{20.8-20.1}{2}=0.35\\
	T'(2)&=\frac{21.5-20.3}{2}=0.6\\
	T'(3)&=\frac{22.6-20.8}{2}=0.9\\
	T'(4)&=\frac{24.2-21.5}{2}=1.35\\
	T'(5)&=\frac{26.1-22.6}{2}=1.75\\
	T'(6)&=\frac{28.5-24.2}{2}=2.15\\
	T'(7)&=\frac{28.5-26.1}{1}=2.4
\end{aligned}
\]

\[
\boxed{T' = [0.2,\ 0.35,\ 0.6,\ 0.9,\ 1.35,\ 1.75,\ 2.15,\ 2.4]\ \text{°C/s}}
\]

---

\textbf{2) Aceleración del cambio (segunda derivada)}

\[
T''(t) \approx \frac{T(t+h)-2T(t)+T(t-h)}{h^2}
\]

\[
\begin{aligned}
	T''(1) &= 20.8 - 2(20.3) + 20.1 = 0.3\\
	T''(2) &= 21.5 - 2(20.8) + 20.3 = 0.2\\
	T''(3) &= 22.6 - 2(21.5) + 20.8 = 0.4\\
	T''(4) &= 24.2 - 2(22.6) + 21.5 = 0.5\\
	T''(5) &= 26.1 - 2(24.2) + 22.6 = 0.3\\
	T''(6) &= 28.5 - 2(26.1) + 24.2 = 0.5
\end{aligned}
\]

\[
\boxed{T'' = [\text{NA},\ 0.3,\ 0.2,\ 0.4,\ 0.5,\ 0.3,\ 0.5,\ \text{NA}]}
\]

---

\textbf{3) Alertas de temperatura (cuando \(T'(t) > 0.8\ °C/s\))}

\[
T'(t) > 0.8 \Rightarrow t = 3,4,5,6,7
\]

\[
\boxed{\text{Alerta desde } t=3\text{ s hasta } t=7\text{ s}}
\]

---

\textbf{4) Normalización (Min-Max Scaling)}

Para cada feature derivada \(x_i\):

\[
x' = \frac{x_i - \min(x)}{\max(x) - \min(x)}
\]

\[
\begin{aligned}
	\min(T') &= 0.2,\ \max(T') = 2.4\\
	T'_{norm} &= \frac{T'-0.2}{2.4-0.2} = \frac{T'-0.2}{2.2}\\
	\Rightarrow T'_{norm} = [0.0,\ 0.068,\ 0.182,\ 0.318,\ 0.523,\ 0.682,\ 0.875,\ 1.0]
\end{aligned}
\]

Para la segunda derivada (ignorando extremos):

\[
\min(T'') = 0.2,\ \max(T'') = 0.5
\]
\[
T''_{norm} = \frac{T''-0.2}{0.3}
\Rightarrow [\text{NA},\ 0.33,\ 0.0,\ 0.67,\ 1.0,\ 0.33,\ 1.0,\ \text{NA}]
\]

---

\textbf{5) Interpretación (utilidad en modelos de clasificación)}

Las features derivadas (\(T'\) y \(T''\)) capturan no solo el nivel absoluto de la temperatura, sino también:

- **Cambios rápidos (gradiente):** ayudan a detectar aumentos anómalos incluso si la temperatura absoluta sigue dentro del rango normal.  
- **Patrones de aceleración:** la segunda derivada permite distinguir un incremento sostenido (problema) de una fluctuación puntual.  
- En modelos de clasificación (por ejemplo, detección de anomalías o mantenimiento predictivo), estas características enriquecen la representación temporal, permitiendo al modelo aprender dinámicas en lugar de solo estados estáticos.

\[
\boxed{\text{Las derivadas actúan como detectores de tendencia y estabilidad del sistema.}}
\]

\textbf{Código en R}

\begin{rcode}
# Datos
tiempo <- 0:7
temp <- c(20.1, 20.3, 20.8, 21.5, 22.6, 24.2, 26.1, 28.5)
h <- 1

# 1. Primera derivada (diferencias finitas)
vel <- numeric(length(temp))
for (i in seq_along(temp)) {
	if (i == 1) vel[i] <- (temp[i+1] - temp[i]) / h
	else if (i == length(temp)) vel[i] <- (temp[i] - temp[i-1]) / h
	else vel[i] <- (temp[i+1] - temp[i-1]) / (2*h)
}

# 2. Segunda derivada
acc <- rep(NA, length(temp))
for (i in 2:(length(temp)-1)) {
	acc[i] <- (temp[i+1] - 2*temp[i] + temp[i-1]) / (h^2)
}

# 3. Alertas (cuando velocidad > 0.8 °C/s)
alerta <- tiempo[vel > 0.8]

# 4. Normalización min-max
minmax <- function(x) (x - min(x, na.rm=TRUE)) / (max(x, na.rm=TRUE) - min(x, na.rm=TRUE))
vel_norm <- minmax(vel)
acc_norm <- minmax(acc)

# Mostrar resultados
df <- data.frame(tiempo, temp, vel, acc, vel_norm, acc_norm)
print(round(df, 3))

cat("\nMomentos con alerta (>0.8°C/s):", alerta, "segundos\n")
\end{rcode}

\textbf{Ejecución}

\begin{rcode}
> # Datos
> tiempo <- 0:7
> temp <- c(20.1, 20.3, 20.8, 21.5, 22.6, 24.2, 26.1, 28.5)
> h <- 1
> 
> # 1. Primera derivada (diferencias finitas)
> vel <- numeric(length(temp))
> for (i in seq_along(temp)) {
	+   if (i == 1) vel[i] <- (temp[i+1] - temp[i]) / h
	+   else if (i == length(temp)) vel[i] <- (temp[i] - temp[i-1]) / h
	+   else vel[i] <- (temp[i+1] - temp[i-1]) / (2*h)
	+ }
> 
> # 2. Segunda derivada
> acc <- rep(NA, length(temp))
> for (i in 2:(length(temp)-1)) {
	+   acc[i] <- (temp[i+1] - 2*temp[i] + temp[i-1]) / (h^2)
	+ }
> 
> # 3. Alertas (cuando velocidad > 0.8 °C/s)
> alerta <- tiempo[vel > 0.8]
> 
> # 4. Normalización min-max
> minmax <- function(x) (x - min(x, na.rm=TRUE)) / (max(x, na.rm=TRUE) - min(x, na.rm=TRUE))
> vel_norm <- minmax(vel)
> acc_norm <- minmax(acc)
> 
> # Mostrar resultados
> df <- data.frame(tiempo, temp, vel, acc, vel_norm, acc_norm)
> print(round(df, 3))
tiempo temp  vel acc vel_norm acc_norm
1      0 20.1 0.20  NA    0.000       NA
2      1 20.3 0.35 0.3    0.068    0.333
3      2 20.8 0.60 0.2    0.182    0.000
4      3 21.5 0.90 0.4    0.318    0.667
5      4 22.6 1.35 0.5    0.523    1.000
6      5 24.2 1.75 0.3    0.705    0.333
7      6 26.1 2.15 0.5    0.886    1.000
8      7 28.5 2.40  NA    1.000       NA
> 
> cat("\nMomentos con alerta (>0.8°C/s):", alerta, "segundos\n")

Momentos con alerta (>0.8°C/s): 3 4 5 6 7 segundos
\end{rcode}

\section{Conclusiones}

La diferenciación numérica constituye una herramienta indispensable en las matemáticas aplicadas y la ingeniería. Su fundamento en series de Taylor, su flexibilidad en el uso de datos discretos y su integración con métodos de simulación y optimización la convierten en un componente esencial de la computación científica moderna. Su correcta aplicación requiere comprender tanto el comportamiento del error como la estabilidad de las fórmulas de aproximación \parencite{burden2016, chapra2015, atkinson2009}.

\chapter{Interpolación}

\section{Introducción}

La interpolación es una técnica fundamental del análisis numérico cuyo objetivo es construir una función que pase exactamente por un conjunto de puntos conocidos. A partir de datos discretos, se busca obtener una función aproximante que permita estimar valores intermedios, suavizar información o servir como base para futuros cálculos numéricos \parencite{burden2016, chapra2015}.

La interpolación es ampliamente utilizada en ingeniería, computación científica, procesamiento de datos, ciencias naturales, economía y muchas otras áreas donde los datos experimentales o simulados son comunes. Su fundamento matemático se basa en la existencia de un polinomio único de grado menor o igual a $n$ que pasa por $n+1$ puntos distintos \parencite{anton2012}.

\section{Fundamentos teóricos}

\subsection{Definición del problema}

Dado un conjunto de datos:

\[
(x_0, y_0), \; (x_1, y_1), \; \dots, \; (x_n, y_n)
\]

donde todos los $x_i$ son distintos, se desea encontrar una función $P(x)$ tal que:

\[
P(x_i) = y_i, \qquad i = 0,1,\dots,n.
\]

El tipo de función más comúnmente utilizado es un **polinomio interpolante**, debido a sus propiedades analíticas y su simplicidad computacional \parencite{atkinson2009}.

\section{Interpolación polinómica}

\subsection{Polinomio interpolante}

Existe un único polinomio de grado a lo sumo $n$ que interpola los $n+1$ puntos dados. Este resultado se conoce como el *Teorema de Interpolación Polinómica* \parencite{burden2016}.

Existen varias formas de construir este polinomio, cada una con ventajas computacionales específicas.

\section{Método de interpolación de Lagrange}

\subsection{Polinomio de Lagrange}

El polinomio de Lagrange se construye como:

\[
P(x) = \sum_{i=0}^n y_i L_i(x),
\]

donde:

\[
L_i(x) = \prod_{\substack{j=0 \\ j\neq i}}^{n} \frac{x - x_j}{x_i - x_j}.
\]

Cada $L_i(x)$ es un polinomio base que vale 1 en $x=x_i$ y 0 en los demás nodos.

Este método es útil teóricamente y tiene gran valor conceptual, aunque computacionalmente no es el más eficiente cuando se añaden nuevos puntos \parencite{burden2016, riley2006}.

\subsection{Error en el polinomio de Lagrange}

El error de interpolación está dado por la expresión:

\[
f(x) - P(x) =
\frac{f^{(n+1)}(\xi)}{(n+1)!}
\prod_{i=0}^n (x - x_i),
\]

para algún $\xi$ en el intervalo que contiene los nodos.  
Este resultado permite analizar cómo la distancia entre nodos y el grado del polinomio afectan la precisión \parencite{burden2016}.

\section{Interpolación por diferencias divididas de Newton}

\subsection{Diferencias divididas}

Las diferencias divididas son definidas recursivamente como:

\[
f[x_i] = y_i,
\]

\[
f[x_i, x_{i+1}] = \frac{f[x_{i+1}] - f[x_i]}{x_{i+1} - x_i},
\]

\[
f[x_i, x_{i+1}, x_{i+2}] = 
\frac{f[x_{i+1}, x_{i+2}] - f[x_i, x_{i+1}]}{x_{i+2} - x_i},
\]

y así sucesivamente.

\subsection{Polinomio de Newton}

El polinomio interpolante toma la forma:

\[
P(x) = a_0 + a_1 (x-x_0) + a_2 (x-x_0)(x-x_1)
+ \cdots + a_n \prod_{i=0}^{n-1} (x-x_i),
\]

donde los coeficientes $a_k$ son diferencias divididas:

\[
a_k = f[x_0, x_1, \dots, x_k].
\]

Esta formulación permite actualizar el polinomio fácilmente cuando se agrega un nuevo punto, lo cual la vuelve más eficiente que Lagrange en aplicaciones prácticas \parencite{chapra2015, burden2016}.

\subsection{Error del polinomio de Newton}

El error del polinomio de Newton coincide con el de Lagrange, pues ambos representan el mismo polinomio de interpolación:

\[
f(x) - P(x) =
\frac{f^{(n+1)}(\xi)}{(n+1)!}
\prod_{i=0}^{n} (x - x_i).
\]

\section{Problemas del polinomio global}

\subsection{Fenómeno de Runge}

Cuando se utilizan muchos puntos igualmente espaciados, el polinomio interpolante puede presentar oscilaciones significativas en los extremos del intervalo. Este comportamiento se conoce como el *fenómeno de Runge* y constituye una limitación importante de la interpolación polinómica global \parencite{atkinson2009, cheney2009}.

\subsection{Crecimiento del error para grados altos}

El error tiende a aumentar rápidamente cuando se incrementa el grado del polinomio debido a:

\begin{itemize}
	\item oscilaciones del polinomio,
	\item mala condición numérica,
	\item aumento de la magnitud de los términos del error.
\end{itemize}

Por este motivo, en muchas aplicaciones se prefiere dividir el intervalo en partes más pequeñas \parencite{burden2016}.

\section{Interpolación por tramos: splines}

\subsection{Concepto de spline}

Un *spline* es una función definida por tramos, donde cada tramo es típicamente un polinomio de bajo grado (por lo general grado 3). Su objetivo es evitar las oscilaciones del polinomio global manteniendo a la vez alta suavidad y precisión \parencite{press2007}.

\subsection{Spline cúbico}

El spline cúbico satisface:

\[
S_i(x_i) = y_i, \qquad S_i(x_{i+1}) = y_{i+1},
\]

y se impone que la función sea:

\begin{itemize}
	\item continua,
	\item con derivada primera continua,
	\item con derivada segunda continua,
\end{itemize}

en cada nodo.

Los splines son ampliamente utilizados en gráficos por computadora, diseño asistido por computadora, procesamiento de señales y simulaciones científicas \parencite{press2007, chapra2015}.

\section{Aplicaciones de la interpolación}

La interpolación es esencial en numerosas áreas:

\begin{itemize}
	\item reconstrucción de funciones a partir de datos discretos,
	\item procesamiento de señales,
	\item modelado y simulación numérica,
	\item gráficos y animación por computadora,
	\item soluciones aproximadas de ecuaciones diferenciales,
	\item generación de funciones suaves para análisis y optimización.
\end{itemize}

En ingeniería, por ejemplo, se utiliza para calibrar instrumentos, construir curvas de rendimiento o estimar valores no medidos experimentalmente \parencite{chapra2015, burden2016}.

\section{Conclusiones}

La interpolación constituye una herramienta esencial para el análisis de datos y la aproximación de funciones. Los métodos clásicos como Lagrange y Newton proporcionan una base sólida, mientras que los splines ofrecen mayor estabilidad y suavidad. La elección del método depende del número de puntos, la distribución de los mismos y la precisión requerida \parencite{burden2016, atkinson2009}.

\chapter{Valores y Vectores Propios}

\section{ ¿Qué son y por qué importan?}

Cuando estudiamos álgebra lineal, aprendemos que una matriz cuadrada $\mathbf{A}$ actúa como una función que transforma vectores. Generalmente, cuando una matriz multiplica a un vector, el resultado es un nuevo vector que ha cambiado tanto de dirección como de longitud.

Sin embargo, para casi todas las transformaciones lineales, existen ciertos vectores especiales que poseen una propiedad extraordinaria: no cambian su dirección al ser transformados por la matriz.

Imaginemos, por ejemplo, la rotación de un globo terráqueo. Casi todos los puntos en la superficie del globo se mueven a una nueva posición cuando este gira. Sin embargo, los puntos que están sobre el eje de rotación (el Polo Norte y el Polo Sur) no se desplazan lateralmente; permanecen sobre la misma línea. En el lenguaje del álgebra lineal, el eje de rotación representa un vector propio de esa transformación.

El término eigen proviene del alemán y significa "propio", "característico" o "innato". Por ello, a menudo se les llama valores y vectores característicos. Su importancia radica en que nos revelan los "ejes principales" o la estructura interna oculta de la matriz, simplificando problemas complejos de dinámica, vibraciones y análisis de datos \parencite{strang2016}.

\section{La transformación fundamental}

Matemáticamente, esta relación especial se define mediante una de las ecuaciones más famosas del álgebra lineal.

Sea $\mathbf{A}$ una matriz cuadrada de tamaño $n \times n$. Decimos que un vector $\mathbf{v}$ (distinto de cero) es un \textbf{vector propio} de $\mathbf{A}$ si se cumple la siguiente igualdad:

\[
\mathbf{A} \mathbf{v} = \lambda \mathbf{v}
\]

Donde:
\begin{itemize}
	\item $\mathbf{A}$ es la \textbf{matriz de transformación}. Representa la operación que estamos aplicando (rotación, estiramiento, corte, etc.).
	\item $\mathbf{v}$ es el \textbf{vector propio} (o eigenvector). Es el vector que mantiene su dirección original. Es importante notar que $\mathbf{v}$ no puede ser el vector nulo ($\mathbf{0}$).
	\item $\lambda$ (lambda) es el \textbf{valor propio} (o eigenvalor). Es un escalar (un número real o complejo) que indica cuánto se "estira" o "encoge" el vector $\mathbf{v}$.
\end{itemize}

\subsubsection{Interpretación geométrica}

La ecuación $\mathbf{A} \mathbf{v} = \lambda \mathbf{v}$ nos dice que la acción de la matriz $\mathbf{A}$ sobre el vector $\mathbf{v}$ es equivalente a simplemente multiplicar el vector por el número $\lambda$.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\linewidth]{Figuras/eigenvectors.png}
	\caption{Comparación de transformaciones. A la izquierda, un vector común cambia de dirección. A la derecha, un vector propio mantiene su dirección, solo cambia su longitud.}
	\label{fig:eigen_concept}
\end{figure}

Dependiendo del valor de $\lambda$, el efecto sobre el vector propio $\mathbf{v}$ puede ser:
\begin{itemize}
	\item Si $\lambda > 1$: El vector se estira.
	\item Si $0 < \lambda < 1$: El vector se contrae.
	\item Si $\lambda < 0$: El vector invierte su sentido (apunta al lado opuesto), pero se mantiene sobre la misma línea de acción.
	\item Si $\lambda = 0$: El vector se colapsa al origen (el sistema pierde una dimensión en esa dirección).
\end{itemize}

En resumen, los vectores propios son las "líneas de fuerza" naturales de la matriz, y los valores propios nos dicen con qué intensidad actúa la matriz sobre esas líneas \parencite{anton2012, riley2006}.

\section{Proceso de cálculo}

Calcular valores y vectores propios puede parecer un procedimiento mecánico, pero cada paso tiene una justificación lógica basada en la invertibilidad de las matrices. A continuación, desglosamos el algoritmo general para una matriz cuadrada $\mathbf{A}$ de tamaño $n \times n$.

\subsection{Paso 1: La ecuación característica}

Partimos de la definición fundamental:

\[
\mathbf{A} \mathbf{v} = \lambda \mathbf{v}.
\]

Para resolver esta ecuación, necesitamos agrupar los términos que contienen a $\mathbf{v}$ en un solo lado de la igualdad. Sin embargo, no podemos restar simplemente un escalar $\lambda$ de una matriz $\mathbf{A}$. Para hacerlos compatibles, multiplicamos $\lambda$ por la matriz identidad $\mathbf{I}$:

\[
\mathbf{A} \mathbf{v} - \lambda \mathbf{I} \mathbf{v} = \mathbf{0},
\]
\[
(\mathbf{A} - \lambda \mathbf{I}) \mathbf{v} = \mathbf{0}.
\]

Esta última expresión es un sistema de ecuaciones lineales homogéneo. Aquí surge un punto crucial:

\begin{itemize}
	\item Si la matriz $(\mathbf{A} - \lambda \mathbf{I})$ tuviera inversa, la única solución posible sería $\mathbf{v} = \mathbf{0}$ (la solución trivial).
	\item Como buscamos vectores propios \textbf{no nulos} ($\mathbf{v} \neq \mathbf{0}$), la matriz $(\mathbf{A} - \lambda \mathbf{I})$ debe ser \textbf{singular} (no invertible).
\end{itemize}

En álgebra lineal, una matriz es singular si y solo si su determinante es cero. Esto nos lleva a la \textbf{ecuación característica}:

\[
\det(\mathbf{A} - \lambda \mathbf{I}) = 0.
\]

Resolver esta ecuación es la clave para encontrar los valores propios \parencite{anton2012}

\subsection{Paso 2: Cálculo de los valores propios ($\lambda$)}

Al calcular el determinante $\det(\mathbf{A} - \lambda \mathbf{I})$, obtendremos un polinomio en función de $\lambda$ de grado $n$, conocido como el \textbf{polinomio característico}:

\[
p(\lambda) = (-1)^n \lambda^n + c_{n-1}\lambda^{n-1} + \dots + c_0 = 0.
\]

\textbf{Procedimiento:}
\begin{enumerate}
	\item Calcular el determinante de la matriz $(\mathbf{A} - \lambda \mathbf{I})$.
	\item Igualar el resultado a cero.
	\item Encontrar las raíces del polinomio (resolver para $\lambda$).
\end{enumerate}

Estas raíces son los valores propios de la matriz. Según el Teorema Fundamental del Álgebra, una matriz de $n \times n$ tendrá exactamente $n$ valores propios (contando sus multiplicidades y posibles valores complejos) \parencite{burden2016}.

\subsection{Paso 3: Cálculo de los vectores propios ($\mathbf{v}$)}

Una vez conocidos los valores de $\lambda$, debemos encontrar los vectores asociados a cada uno. Para cada valor propio $\lambda_i$ encontrado:

\begin{enumerate}
	\item Sustituimos $\lambda_i$ en la matriz $(\mathbf{A} - \lambda_i \mathbf{I})$.
	\item Resolvemos el sistema homogéneo:
	\[
	(\mathbf{A} - \lambda_i \mathbf{I}) \mathbf{v} = \mathbf{0}.
	\]
	\item El sistema tendrá infinitas soluciones (debido a que el determinante es cero). Debemos expresar la solución general en términos de vectores base.
\end{enumerate}

El conjunto de todos los vectores que satisfacen esta ecuación (más el vector cero) forma el \textbf{espacio propio} (o eigen-espacio) asociado a $\lambda_i$ .

\section{Ejemplo guiado}

Apliquemos el proceso a una matriz $2 \times 2$ para ilustrar los pasos. Sea:

\[
\mathbf{A} = \begin{pmatrix} 3 & 2 \\ 1 & 4 \end{pmatrix}.
\]

\subsection*{1. Construcción de la matriz característica}
Restamos $\lambda$ de la diagonal principal:

\[
\mathbf{A} - \lambda \mathbf{I} = \begin{pmatrix} 3-\lambda & 2 \\ 1 & 4-\lambda \end{pmatrix}.
\]

\subsection*{2. Polinomio característico}
Calculamos el determinante e igualamos a cero:

\[
(3-\lambda)(4-\lambda) - (2)(1) = 0,
\]
\[
12 - 3\lambda - 4\lambda + \lambda^2 - 2 = 0,
\]
\[
\lambda^2 - 7\lambda + 10 = 0.
\]

\subsection*{3. Hallar las raíces ($\lambda$)}
Factorizamos la ecuación cuadrática:

\[
(\lambda - 5)(\lambda - 2) = 0.
\]

Los valores propios son \textbf{$\lambda_1 = 5$} y \textbf{$\lambda_2 = 2$}.

\subsection*{4. Hallar los vectores propios ($\mathbf{v}$)}

Caso $\lambda_1 = 5$
Sustituimos en $(\mathbf{A} - 5\mathbf{I})\mathbf{v} = \mathbf{0}$:

\[
\begin{pmatrix} -2 & 2 \\ 1 & -1 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}.
\]

Esto nos da la ecuación $-2x + 2y = 0$, o simplificando, $x = y$. Si elegimos $y=1$, entonces $x=1$.
\[
\mathbf{v}_1 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}.
\]

Caso $\lambda_2 = 2$:
Sustituimos en $(\mathbf{A} - 2\mathbf{I})\mathbf{v} = \mathbf{0}$:

\[
\begin{pmatrix} 1 & 2 \\ 1 & 2 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}.
\]

Esto nos da $x + 2y = 0$, o $x = -2y$. Si elegimos $y=1$, entonces $x=-2$.
\[
\mathbf{v}_2 = \begin{pmatrix} -2 \\ 1 \end{pmatrix}.
\]

\section{Propiedades y diagonalización}

Una de las utilidades más potentes de los valores y vectores propios es la capacidad de simplificar operaciones matriciales complejas. Si una matriz cuadrada puede ser interpretada como una transformación que solo estira vectores en ciertas direcciones, entonces podemos cambiar nuestro sistema de referencia para trabajar únicamente con estos estiramientos simples.

\subsection{Independencia lineal}

Para que una matriz $\mathbf{A}$ de tamaño $n \times n$ pueda ser simplificada completamente, necesita tener un conjunto completo de $n$ vectores propios linealmente independientes. Esto está garantizado siempre que la matriz tenga $n$ valores propios distintos.

En el caso de que existan valores propios repetidos (es decir, una raíz múltiple en el polinomio característico), es posible que no existan suficientes vectores propios independientes. Si esto ocurre, se dice que la matriz es \textit{defectuosa} y no puede diagonalizarse completamente, aunque puede aproximarse mediante la forma canónica de Jordan \parencite{anton2012}.

\subsection{El teorema de diagonalización}

Si la matriz $\mathbf{A}$ tiene $n$ vectores propios linealmente independientes, entonces es \textbf{diagonalizable}. Esto significa que $\mathbf{A}$ puede descomponerse en el producto de tres matrices específicas:

\[
\mathbf{A} = \mathbf{P} \mathbf{D} \mathbf{P}^{-1}
\]

Los componentes de esta factorización son:

\begin{itemize}
	\item $\mathbf{P}$: La \textbf{matriz de vectores propios}. Se construye colocando los vectores propios $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n$ como columnas de la matriz.
	\item $\mathbf{D}$: La \textbf{matriz diagonal}. Es una matriz donde todos los elementos fuera de la diagonal principal son cero. Los elementos de la diagonal son los valores propios $\lambda_1, \lambda_2, \dots, \lambda_n$, colocados en el mismo orden que sus correspondientes vectores en $\mathbf{P}$.
\end{itemize}

Esta igualdad ($\mathbf{D} = \mathbf{P}^{-1} \mathbf{A} \mathbf{P}$) implica que la matriz $\mathbf{A}$ es semejante a la matriz diagonal $\mathbf{D}$.

\subsection{Aplicación: Potencia de matrices}

Una consecuencia práctica inmediata de la diagonalización es el cálculo eficiente de potencias de matrices. Calcular $\mathbf{A}^k$ multiplicando la matriz por sí misma repetidamente es computacionalmente costoso ($O(n^3)$ por multiplicación) y propenso a acumular errores de redondeo.

Utilizando la descomposición espectral, observamos que:

\[
\mathbf{A}^2 = (\mathbf{P} \mathbf{D} \mathbf{P}^{-1}) (\mathbf{P} \mathbf{D} \mathbf{P}^{-1}) = \mathbf{P} \mathbf{D} (\mathbf{P}^{-1} \mathbf{P}) \mathbf{D} \mathbf{P}^{-1} = \mathbf{P} \mathbf{D}^2 \mathbf{P}^{-1}.
\]

Generalizando para cualquier potencia $k$:

\[
\mathbf{A}^k = \mathbf{P} \mathbf{D}^k \mathbf{P}^{-1}.
\]

La ventaja radica en que elevar una matriz diagonal a una potencia es trivial: simplemente se eleva cada elemento de la diagonal a la potencia $k$.

\[
\mathbf{D}^k = \begin{pmatrix}
	\lambda_1^k & 0 & \cdots & 0 \\
	0 & \lambda_2^k & \cdots & 0 \\
	\vdots & \vdots & \ddots & \vdots \\
	0 & 0 & \cdots & \lambda_n^k
\end{pmatrix}.
\]

Este atajo reduce drásticamente el costo de cómputo en algoritmos que requieren iteraciones matriciales, como las cadenas de Markov o las predicciones en sistemas dinámicos discretos \parencite{strang2016}.

\section{Implementación en R}

A continuación, se presenta el script completo para realizar el análisis espectral de la matriz $\mathbf{A}$ y utilizar sus propiedades para calcular potencias grandes de la misma.

\begin{rcode}
	A <- matrix(c(3, 1, 2, 4), nrow = 2, byrow = TRUE)
	
	sistema <- eigen(A)
	lambdas <- sistema$values
	V <- sistema$vectors
	
	print("Valores Propios:")
	print(lambdas)
	print("Vectores Propios:")
	print(V)
	
	k <- 20
	D_k <- diag(lambdas^k)
	P_inv <- solve(V)
	
	A_final <- V 
	
	print("Resultado de A elevado a la 20:")
	print(A_final)
\end{rcode}

\subsection{Explicación del código}

El código anterior se divide en tres etapas lógicas:

\begin{enumerate}
	\item \textbf{Definición y cálculo:} Primero definimos la matriz $\mathbf{A}$. La función nativa \texttt{eigen()} realiza todo el trabajo pesado, devolviendo una lista que separamos en \texttt{lambdas} (valores propios) y \texttt{V} (matriz de vectores propios).
	
	\item \textbf{Preparación de la diagonalización:} Para calcular $\mathbf{A}^{20}$, no multiplicamos la matriz por sí misma 20 veces. En su lugar, aplicamos la propiedad $\mathbf{A}^k = \mathbf{V}\mathbf{D}^k\mathbf{V}^{-1}$.
	\begin{itemize}
		\item Elevamos el vector \texttt{lambdas} a la potencia $k=20$ y lo convertimos en una matriz diagonal (\texttt{D\_k}).
		\item Calculamos la inversa de la matriz de vectores propios (\texttt{solve(V)}).
	\end{itemize}
	
	\item \textbf{Reconstrucción:} Finalmente, multiplicamos las tres matrices componentes ($\mathbf{V} \times \mathbf{D}^k \times \mathbf{V}^{-1}$) para obtener el resultado final de forma eficiente.
\end{enumerate}

\subsection{Visualización de resultados}

Al ejecutar el código, obtenemos los valores propios $\lambda_1 = 5$ y $\lambda_2 = 2$. Como se observa en la Figura \ref{fig:resultados_eigen}, la matriz final contiene valores extremadamente grandes (del orden de $10^{13}$).

Esto ocurre porque el término $5^{20}$ domina completamente la ecuación. Geométricamente, esto significa que cualquier vector transformado repetidamente por $\mathbf{A}$ terminará alineándose con el primer vector propio.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.7\linewidth]{Figuras/ResultadosEigen.png}
	\caption{Salida de la consola en R mostrando los valores propios y la matriz resultante $\mathbf{A}^{20}$.}
	\label{fig:resultados_eigen}
\end{figure}

\section{Eigenvalues y eigenvectores aplicados}

\textbf{Contexto}

En un hospital general, los pacientes hospitalizados transitan entre distintos
servicios médicos, y en cada uno de ellos se les prescribe un tipo de dieta
específica según su condición clínica. Estos desplazamientos no son aleatorios,
sino que responden a protocolos médicos, evolución del estado de salud y
decisiones administrativas del hospital.

Sin embargo, no siempre se dispone de bases de datos completas que describan
con precisión estos flujos. En estos casos, es posible construir un modelo
matemático hipotético que represente de manera razonable el comportamiento del
sistema, permitiendo analizar escenarios y apoyar la toma de decisiones.

En este modelo, se consideran cuatro estados principales del sistema hospitalario:

\begin{itemize}
	\item Emergencia (dieta líquida)
	\item Medicina Interna (dieta blanda)
	\item Cirugía (dieta progresiva)
	\item Alta hospitalaria
\end{itemize}

Cada estado representa la proporción de pacientes que se encuentran en un
determinado servicio médico y, por tanto, bajo un tipo de dieta específico.

\textbf{Planteamiento del modelo}

Se modela el sistema mediante una cadena de Markov de tiempo discreto, donde
las probabilidades de transición representan la posibilidad de que un paciente
pase de un servicio médico a otro en un intervalo de tiempo determinado.
La matriz de transición inicial se construye de forma hipotética, basándose en
criterios clínicos generales y en el flujo típico de pacientes dentro de un
hospital.

\textbf{Tarea}

\begin{enumerate}
	\item Proponer una matriz de transición inicial que represente el flujo de
	pacientes entre los distintos servicios médicos del hospital, asegurando
	que cada fila sume 1.
	
	\item Simular una intervención hospitalaria orientada a fortalecer el
	servicio de Medicina Interna, asumiendo que:
	\begin{itemize}
		\item Aumenta la probabilidad de que pacientes provenientes de Emergencia
		sean derivados a Medicina Interna.
		\item Aumenta la probabilidad de permanencia de los pacientes en Medicina
		Interna.
		\item Disminuye la probabilidad de alta directa desde Medicina Interna.
		\item Las demás probabilidades se ajustan para conservar la coherencia
		del modelo.
	\end{itemize}
	
	\item Calcular los eigenvalues y eigenvectores de la matriz de transición
	modificada.
	
	\item Determinar la distribución estacionaria del sistema y analizar el
	comportamiento de los pacientes a largo plazo.
	
	\item Comparar la distribución estacionaria original y la modificada,
	evaluando el impacto de la intervención hospitalaria sobre la permanencia
	de los pacientes en Medicina Interna y en los demás servicios.
\end{enumerate}

\textbf{Justificación del uso de datos hipotéticos}

La ausencia de una base de datos real no invalida el modelo propuesto, ya que el
objetivo principal es analizar el comportamiento dinámico del sistema bajo
supuestos razonables. La matriz de transición se construye a partir de criterios
clínicos generales, protocolos hospitalarios habituales y consideraciones
operativas, permitiendo estudiar escenarios hipotéticos y evaluar decisiones de
gestión hospitalaria.

\textbf{Preguntas de reflexión}

\begin{itemize}
	\item ¿La intervención en Medicina Interna mejora la eficiencia del flujo de
	pacientes dentro del hospital?
	\item ¿Cómo podría este modelo apoyar la planificación de dietas y recursos
	nutricionales?
	\item ¿Qué implicancias tendría este cambio en la carga de trabajo del
	personal de salud?
\end{itemize}

\textbf{Solución}

1.- Proponer una matriz de transición inicial que represente el flujo de
pacientes entre los distintos servicios médicos del hospital, asegurando
que cada fila sume 1.

2.- Simular una intervención hospitalaria orientada a fortalecer el
servicio de Medicina Interna, asumiendo que:
\begin{itemize}
	\item Aumenta la probabilidad de que pacientes provenientes de Emergencia
	sean derivados a Medicina Interna.
	\item Aumenta la probabilidad de permanencia de los pacientes en Medicina
	Interna.
	\item Disminuye la probabilidad de alta directa desde Medicina Interna.
	\item Las demás probabilidades se ajustan para conservar la coherencia
	del modelo.
\end{itemize}

\textbf{Código en Python}

\begin{pythoncode}
# Importar librerías necesarias
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import linalg
import pandas as pd

# Configuración de visualización
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 10
sns.set_style("whitegrid")

print("Librerías importadas correctamente")
print(f"NumPy versión: {np.__version__}")

# Definición de servicios médicos
servicios = [
'Emergencia\n(Dieta líquida)',
'Medicina Interna\n(Dieta blanda)',
'Cirugía\n(Dieta progresiva)',
'Alta hospitalaria'
]

n_servicios = len(servicios)

# Matriz de transición inicial
# T[i,j] = probabilidad de pasar del servicio i al servicio j
T_inicial = np.array([
[0.20, 0.45, 0.25, 0.10],  # Desde Emergencia
[0.05, 0.40, 0.25, 0.30],  # Desde Medicina Interna
[0.05, 0.15, 0.50, 0.30],  # Desde Cirugía
[0.00, 0.00, 0.00, 1.00]   # Desde Alta hospitalaria
])

# Crear DataFrame
df_T_inicial = pd.DataFrame(
T_inicial,
index=servicios,
columns=servicios
)

print("MATRIZ DE TRANSICIÓN INICIAL")
print("=" * 70)
print(df_T_inicial)

print("\nVerificación: cada fila debe sumar 1")
print("-" * 70)
for i, serv in enumerate(servicios):
suma = T_inicial[i, :].sum()
status = "✓" if abs(suma - 1.0) < 1e-6 else "✗"
print(f"{status} {serv:35}: suma = {suma:.3f}")

plt.figure(figsize=(10, 8))
sns.heatmap(
T_inicial,
annot=True,
fmt='.2f',
cmap='YlGnBu',
xticklabels=servicios,
yticklabels=servicios,
cbar_kws={'label': 'Probabilidad de transición'},
linewidths=0.5,
linecolor='gray'
)

plt.title(
'Matriz de Transición Inicial\nFlujo de Pacientes y Dietas Hospitalarias',
fontsize=14,
fontweight='bold',
pad=20
)
plt.xlabel('Servicio destino', fontsize=12)
plt.ylabel('Servicio origen', fontsize=12)
plt.tight_layout()
plt.show()

print("\nLos valores más altos indican mayor probabilidad de tránsito entre servicios")

# Matriz de transición modificada (intervención hospitalaria)
T_intervencion = np.array([
[0.15, 0.55, 0.20, 0.10],  # Desde Emergencia
[0.05, 0.55, 0.25, 0.15],  # Desde Medicina Interna (más permanencia)
[0.05, 0.15, 0.50, 0.30],  # Desde Cirugía (sin cambios)
[0.00, 0.00, 0.00, 1.00]   # Desde Alta hospitalaria
])

df_T_intervencion = pd.DataFrame(
T_intervencion,
index=servicios,
columns=servicios
)

print("\nMATRIZ DE TRANSICIÓN MODIFICADA (INTERVENCIÓN)")
print("=" * 70)
print(df_T_intervencion)

print("\nVerificación: cada fila debe sumar 1")
print("-" * 70)
for i, serv in enumerate(servicios):
suma = T_intervencion[i, :].sum()
status = "✓" if abs(suma - 1.0) < 1e-6 else "✗"
print(f"{status} {serv:35}: suma = {suma:.3f}")

plt.figure(figsize=(10, 8))
sns.heatmap(
T_intervencion,
annot=True,
fmt='.2f',
cmap='YlOrRd',
xticklabels=servicios,
yticklabels=servicios,
cbar_kws={'label': 'Probabilidad de transición'},
linewidths=0.5,
linecolor='gray'
)

plt.title(
'Matriz de Transición con Intervención\nFortalecimiento de Medicina Interna',
fontsize=14,
fontweight='bold',
pad=20
)
plt.xlabel('Servicio destino', fontsize=12)
plt.ylabel('Servicio origen', fontsize=12)
plt.tight_layout()
plt.show()

print("\nLa intervención incrementa la retención de pacientes en Medicina Interna")
\end{pythoncode}

\textbf{Ejecución}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\linewidth]{Figuras/Matriz1.png}
	\caption{Matriz de transición inicial - Flujo de pacientes y dietas hospitalarias}
	\label{Matriz1}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\linewidth]{Figuras/Matriz2.png}
	\caption{Matriz de transición con intervención - Fortalecimiento de Medicina Interna}
	\label{Matriz2}
\end{figure}

\begin{pythoncode}
Librerías importadas correctamente
NumPy versión: 2.3.3
MATRIZ DE TRANSICIÓN INICIAL
======================================================================
Emergencia\n(Dieta líquida)  ...  Alta hospitalaria
Emergencia\n(Dieta líquida)                              0.20  ...                0.1
Medicina Interna\n(Dieta blanda)                         0.05  ...                0.3
Cirugía\n(Dieta progresiva)                              0.05  ...                0.3
Alta hospitalaria                                        0.00  ...                1.0

[4 rows x 4 columns]

Verificación: cada fila debe sumar 1
----------------------------------------------------------------------
✓ Emergencia
(Dieta líquida)         : suma = 1.000
✓ Medicina Interna
(Dieta blanda)    : suma = 1.000
✓ Cirugía
(Dieta progresiva)         : suma = 1.000
✓ Alta hospitalaria                  : suma = 1.000

Los valores más altos indican mayor probabilidad de tránsito entre servicios

MATRIZ DE TRANSICIÓN MODIFICADA (INTERVENCIÓN)
======================================================================
Emergencia\n(Dieta líquida)  ...  Alta hospitalaria
Emergencia\n(Dieta líquida)                              0.15  ...               0.10
Medicina Interna\n(Dieta blanda)                         0.05  ...               0.15
Cirugía\n(Dieta progresiva)                              0.05  ...               0.30
Alta hospitalaria                                        0.00  ...               1.00

[4 rows x 4 columns]

Verificación: cada fila debe sumar 1
----------------------------------------------------------------------
✓ Emergencia
(Dieta líquida)         : suma = 1.000
✓ Medicina Interna
(Dieta blanda)    : suma = 1.000
✓ Cirugía
(Dieta progresiva)         : suma = 1.000
✓ Alta hospitalaria                  : suma = 1.000

La intervención incrementa la retención de pacientes en Medicina Interna
\end{pythoncode}

3.- Calcular los eigenvalues y eigenvectores de la matriz de transición modificada.

\begin{pythoncode}
# ============================================================
# CÁLCULO DE EIGENVALUES Y EIGENVECTORES
# MATRIZ MODIFICADA (INTERVENCIÓN HOSPITALARIA)
# ============================================================

# Cálculo de eigenvalues y eigenvectors
# Usamos la transpuesta para cadenas de Markov
eigenvalues, eigenvectors = linalg.eig(T_intervencion.T)

print("\nEIGENVALUES ENCONTRADOS")
print("=" * 70)

for i, val in enumerate(eigenvalues):
if np.isreal(val):
print(f"  λ_{i+1} = {val.real:8.5f}")
else:
print(f"  λ_{i+1} = {val.real:8.5f} + {val.imag:8.5f}i")

# Identificar eigenvalue dominante (|λ| máximo)
idx_dominante = np.argmax(np.abs(eigenvalues))
lambda_dominante = eigenvalues[idx_dominante]

print("\n" + "=" * 70)
print(f"EIGENVALUE DOMINANTE: λ_{idx_dominante+1} = {lambda_dominante.real:.6f}")

print("\nINTERPRETACIÓN:")
print("   • λ ≈ 1  → Existe un estado estacionario")
print("   • |λ| < 1 → El sistema converge al equilibrio")
print("   • La magnitud del segundo eigenvalue determina la velocidad de convergencia")
print("=" * 70)

# ============================================================
# VISUALIZACIÓN DE EIGENVALUES EN EL PLANO COMPLEJO
# ============================================================

plt.figure(figsize=(10, 8))

# Círculo unitario
theta = np.linspace(0, 2*np.pi, 200)
plt.plot(np.cos(theta), np.sin(theta), 'k--', alpha=0.4, linewidth=1.5)

# Ejes
plt.axhline(0, color='black', linewidth=0.7)
plt.axvline(0, color='black', linewidth=0.7)

# Graficar eigenvalues
for i, val in enumerate(eigenvalues):
if i == idx_dominante:
plt.scatter(val.real, val.imag,
s=300, marker='*',
color='red', edgecolor='black',
linewidth=2, zorder=3,
label='Eigenvalue dominante')
else:
plt.scatter(val.real, val.imag,
s=150, alpha=0.7,
edgecolor='black', linewidth=1.2)

plt.annotate(f'λ_{i+1}',
(val.real, val.imag),
xytext=(8, 8),
textcoords='offset points',
fontsize=10, fontweight='bold')

plt.xlabel('Parte real', fontsize=12, fontweight='bold')
plt.ylabel('Parte imaginaria', fontsize=12, fontweight='bold')
plt.title('Eigenvalues de la Matriz de Transición Modificada',
fontsize=14, fontweight='bold', pad=15)

plt.grid(True, alpha=0.3)
plt.axis('equal')
plt.legend()
plt.tight_layout()
plt.show()
\end{pythoncode}

\textbf{Ejecución}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\linewidth]{Figuras/Eigenvalores.png}
	\caption{Eigenvalores de la matriz de transición modificada}
	\label{Eigenvalores}
\end{figure}

\begin{pythoncode}
EIGENVALUES ENCONTRADOS
======================================================================
λ_1 =  1.00000
λ_2 =  0.78452
λ_3 =  0.31548
λ_4 =  0.10000

======================================================================
EIGENVALUE DOMINANTE: λ_1 = 1.000000

INTERPRETACIÓN:
• λ ≈ 1  → Existe un estado estacionario
• |λ| < 1 → El sistema converge al equilibrio
• La magnitud del segundo eigenvalue determina la velocidad de convergencia
======================================================================
\end{pythoncode}

4.- Determinar la distribución estacionaria del sistema y analizar el comportamiento de los pacientes a largo plazo.

\textbf{Código en Python}

\begin{pythoncode}
# ============================================================
# DISTRIBUCIÓN ESTACIONARIA DEL SISTEMA HOSPITALARIO
# ============================================================

# Definición de servicios médicos del hospital
destinos = [
'Emergencia',
'Medicina Interna',
'Cirugía',
'Alta'
]

n_destinos = len(destinos)

# Extraer eigenvector asociado al eigenvalue dominante
v_dominante = eigenvectors[:, idx_dominante].real

# Normalizar para que sume 1 (distribución de probabilidad)
dist_estacionaria = v_dominante / v_dominante.sum()

print("\nDISTRIBUCIÓN ESTACIONARIA DE PACIENTES")
print("=" * 70)
print("\nDistribución de equilibrio (largo plazo):")
print("-" * 70)

for i, servicio in enumerate(destinos):
porcentaje = dist_estacionaria[i] * 100
barra = "█" * int(porcentaje / 2)
print(f"{servicio:20} : {porcentaje:6.2f}% {barra}")

# Identificar servicio dominante
idx_hub = np.argmax(dist_estacionaria)

print("\n" + "=" * 70)
print(f"SERVICIO DOMINANTE DEL SISTEMA: {destinos[idx_hub]}")
print(f"   Concentra aproximadamente el {dist_estacionaria[idx_hub]*100:.2f}%")
print("=" * 70)

# ============================================================
# VISUALIZACIÓN DE LA DISTRIBUCIÓN ESTACIONARIA
# ============================================================

fig, ax = plt.subplots(figsize=(10, 6))

barras = ax.bar(destinos,
dist_estacionaria * 100,
alpha=0.75,
edgecolor='black',
linewidth=2)

ax.set_ylabel('Porcentaje de Pacientes (%)',
fontsize=12, fontweight='bold')
ax.set_title('Distribución Estacionaria de Pacientes\n(Equilibrio a Largo Plazo)',
fontsize=14, fontweight='bold', pad=15)

ax.grid(axis='y', alpha=0.3)
ax.set_ylim([0, max(dist_estacionaria * 100) * 1.2])

# Valores sobre las barras
for barra in barras:
altura = barra.get_height()
ax.text(barra.get_x() + barra.get_width()/2,
altura,
f'{altura:.1f}%',
ha='center', va='bottom',
fontsize=11, fontweight='bold')

plt.tight_layout()
plt.show()

# ============================================================
# INTERPRETACIÓN AUTOMÁTICA
# ============================================================

print("\nINTERPRETACIÓN DEL COMPORTAMIENTO A LARGO PLAZO:")
print("   • El estado 'Alta' actúa como un estado absorbente del sistema.")
print("   • A largo plazo, el 100% de los pacientes termina recibiendo el alta médica.")
print("   • La distribución estacionaria representa el destino final de los pacientes,")
print("     no la carga hospitalaria ni la ocupación de servicios.")
print("   • Para analizar impacto en Medicina Interna, es necesario estudiar")
print("     la evolución transitoria o redefinir 'Alta' como un estado no absorbente.")
\end{pythoncode}

\textbf{Ejecución}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\linewidth]{Figuras/Distribución.png}
	\caption{Distribución estacionaria de pacientes (Equilibrio a largo plazo)}
	\label{Distribución}
\end{figure}

\begin{pythoncode}
DISTRIBUCIÓN ESTACIONARIA DE PACIENTES
======================================================================

Distribución de equilibrio (largo plazo):
----------------------------------------------------------------------
Emergencia           :   0.00%
Medicina Interna     :   0.00%
Cirugía              :   0.00%
Alta                 : 100.00% ██████████████████████████████████████████████████

======================================================================
SERVICIO DOMINANTE DEL SISTEMA: Alta
Concentra aproximadamente el 100.00%
======================================================================

INTERPRETACIÓN DEL COMPORTAMIENTO A LARGO PLAZO:
• El estado 'Alta' actúa como un estado absorbente del sistema.
• A largo plazo, el 100% de los pacientes termina recibiendo el alta médica.
• La distribución estacionaria representa el destino final de los pacientes,
no la carga hospitalaria ni la ocupación de servicios.
• Para analizar impacto en Medicina Interna, es necesario estudiar
la evolución transitoria o redefinir 'Alta' como un estado no absorbente.
\end{pythoncode}

5.- Comparar la distribución estacionaria original y la modificada, evaluando el impacto de la intervención hospitalaria sobre la permanencia de los pacientes en Medicina Interna y los demás servicios.

\textbf{Comparación de la distribución estacionaria original y modificada}

La Tabla \ref{tab:comparacion_estacionaria} muestra la comparación entre la
distribución estacionaria del modelo hospitalario original y la obtenida tras
la intervención orientada a fortalecer el servicio de Medicina Interna.

\begin{table}[H]
	\centering
	\caption{Distribución estacionaria antes y después de la intervención hospitalaria}
	\label{tab:comparacion_estacionaria}
	\begin{tabular}{lcc}
		\hline
		\textbf{Servicio} & \textbf{Modelo original (\%)} & \textbf{Modelo modificado (\%)} \\
		\hline
		Emergencia         & 0.00 & 0.00 \\
		Medicina Interna   & 0.00 & 0.00 \\
		Cirugía            & 0.00 & 0.00 \\
		Alta               & 100.00 & 100.00 \\
		\hline
	\end{tabular}
\end{table}

\textbf{Análisis del impacto de la intervención}

La comparación de ambas distribuciones estacionarias muestra que, en los dos
escenarios, el sistema converge completamente al estado \emph{Alta}, el cual
funciona como un estado absorbente del modelo. Esto implica que, a largo plazo,
el 100\% de los pacientes termina recibiendo el alta médica, independientemente
de la intervención aplicada.

Desde el punto de vista matemático, la intervención hospitalaria no altera la
distribución estacionaria del sistema, ya que esta distribución refleja el
\textbf{destino final de los pacientes} y no el tiempo que permanecen en cada
servicio.

Sin embargo, esto no significa que la intervención sea irrelevante. El aumento
en la probabilidad de derivación y permanencia en Medicina Interna afecta el
\textbf{comportamiento transitorio del sistema}, es decir, la trayectoria que
siguen los pacientes antes de alcanzar el alta. En particular:

\begin{itemize}
	\item Se espera que los pacientes pasen más tiempo en Medicina Interna.
	\item Se modifica la carga temporal de los servicios clínicos.
	\item La intervención puede impactar en la demanda de camas, personal y
	recursos, aunque esto no se refleje en la distribución estacionaria.
\end{itemize}

Por lo tanto, para evaluar adecuadamente el impacto de la intervención sobre la
permanencia en Medicina Interna, resulta más apropiado analizar la evolución
temporal del sistema o los tiempos esperados de permanencia, en lugar de basarse
exclusivamente en la distribución estacionaria.

\textbf{Preguntas de reflexión}

1.- ¿La intervención en Medicina Interna mejora la eficiencia del flujo de pacientes dentro del hospital?


Desde el punto de vista del modelo matemático propuesto, la intervención en
Medicina Interna no modifica la distribución estacionaria del sistema, ya que
todos los pacientes terminan alcanzando el estado de \emph{Alta}, el cual actúa
como un estado absorbente. Por lo tanto, en términos de resultados finales, la
eficiencia global medida por la proporción de pacientes dados de alta permanece
inalterada.

No obstante, la intervención sí mejora la eficiencia del \textbf{flujo interno}
de pacientes durante la evolución temporal del sistema. El aumento en la
probabilidad de derivación desde Emergencia hacia Medicina Interna, así como la
mayor permanencia en dicho servicio, contribuyen a una redistribución más
ordenada de los pacientes entre los servicios clínicos antes del alta.

En este sentido, la eficiencia se manifiesta en aspectos operativos como:
\begin{itemize}
	\item Mejor canalización de pacientes desde Emergencia.
	\item Reducción de derivaciones innecesarias a otros servicios.
	\item Mayor estabilidad en la atención clínica dentro de Medicina Interna.
\end{itemize}

Por lo tanto, aunque la intervención no altera el resultado final del sistema,
sí puede considerarse una mejora en la eficiencia operativa del flujo hospitalario,
especialmente en el corto y mediano plazo.

2.- ¿Cómo podría este modelo apoyar la planificación de dietas y recursos nutricionales?

El modelo de cadenas de Markov permite estimar la \textbf{distribución esperada de
	pacientes en cada servicio hospitalario a lo largo del tiempo}, lo cual resulta
fundamental para la planificación de dietas y recursos nutricionales.

Durante la evolución temporal del sistema, el modelo muestra cómo los pacientes
transitan entre Emergencia, Medicina Interna, Cirugía y Alta, permitiendo
identificar qué servicios concentran una mayor carga asistencial antes del alta.
Esta información puede utilizarse para:

\begin{itemize}
	\item Estimar la demanda diaria de dietas hospitalarias por servicio.
	\item Ajustar la producción de alimentos según la permanencia esperada de los pacientes.
	\item Planificar insumos nutricionales específicos (dietas blandas, hiposódicas, enterales, etc.).
	\item Optimizar la asignación del personal de nutrición clínica.
\end{itemize}

En particular, el fortalecimiento del servicio de Medicina Interna incrementa el
tiempo de permanencia de los pacientes en dicho servicio, lo que implica una mayor
demanda de dietas terapéuticas continuas y seguimiento nutricional. El modelo
permite anticipar este efecto y adecuar los recursos antes de que ocurran cuellos
de botella.

De esta manera, aunque el sistema converge finalmente al alta, el análisis
transitorio proporciona una herramienta cuantitativa para la \textbf{gestión
	proactiva de la alimentación hospitalaria}, mejorando la eficiencia operativa y la
calidad de la atención nutricional.

3.- ¿Qué implicancias tendría este cambio en la carga de trabajo del personal de salud?

La intervención hospitalaria orientada a fortalecer el servicio de Medicina Interna
modifica principalmente la \textbf{dinámica temporal del flujo de pacientes}, más que
la distribución final del sistema, que converge al estado de alta. Sin embargo,
este cambio tiene implicancias relevantes en la carga de trabajo del personal de
salud durante el proceso de atención.

El aumento de la probabilidad de permanencia en Medicina Interna implica que los
pacientes permanecen más tiempo bajo supervisión clínica continua, lo que genera:

\begin{itemize}
	\item Mayor demanda de atención médica y de enfermería en Medicina Interna.
	\item Incremento en la carga de seguimiento clínico, monitoreo y registros.
	\item Mayor necesidad de coordinación con servicios de apoyo (nutrición, laboratorio, farmacia).
\end{itemize}

Al mismo tiempo, la reducción de altas directas desde Medicina Interna puede aliviar
momentáneamente la presión sobre los servicios administrativos asociados al egreso,
pero traslada la carga operativa hacia el personal asistencial del servicio.

Desde el punto de vista de la gestión hospitalaria, el modelo permite anticipar
estos cambios y resalta la necesidad de:

\begin{itemize}
	\item Ajustar la dotación de personal en Medicina Interna.
	\item Reorganizar turnos y cargas horarias.
	\item Fortalecer el trabajo interdisciplinario para evitar la saturación del servicio.
\end{itemize}

En conclusión, aunque el sistema no incrementa el número total de pacientes
atendidos a largo plazo, la intervención redistribuye la carga laboral en el
tiempo, haciendo indispensable una planificación adecuada del recurso humano.